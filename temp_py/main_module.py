import numpy as np
import os
from datetime import datetime
import urllib.request
from sklearn.model_selection import train_test_split
from data_generator import SimpleDataGenerator
from visualization import visualize_tsne
from autoanoeval.ADBench.autoanoeval.model_selection_org import run_model_selection_experiment
import argparse
import sys

from sklearn.preprocessing import MinMaxScaler
import pandas as pd

# ì„¤ì •
RANDOM_SEED = 42
ANOMALY_TYPES = ['local', 'cluster', 'global','discrepancy']  # ì›í•˜ëŠ” 3ê°€ì§€ ì´ìƒì¹˜ ìœ í˜•


class Tee(object):
    def __init__(self, *files):
        self.files = files
    def write(self, obj):
        for f in self.files:
            f.write(obj)
            f.flush()  # ë²„í¼ ë°”ë¡œ ë¹„ìš°ê¸°
    def flush(self):
        for f in self.files:
            f.flush()
            
            
# ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜
def download_dataset(url, filename):
    if not os.path.exists(filename):
        print(f"{filename} ë‹¤ìš´ë¡œë“œ ì¤‘...")
        urllib.request.urlretrieve(url, filename)
        print(f"{filename} ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
    else:
        print(f"{filename}ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")
        
        
def load_dataset(dataset_name):
    csv_path = f'/lab-di/nfsdata/home/suhee.yoon/autoanoeval/data/adbench_column/{dataset_name}.csv'    
    print(f"ğŸ“¥ csv dataset ë¡œë“œ: {csv_path}")
    df = pd.read_csv(csv_path)

    y = df['label'].values
    df = df.drop(columns=['label'])  # label ì œê±°

    # âœ… ìˆ«ì / ë¬¸ìì—´ feature ìë™ êµ¬ë¶„
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    string_cols = df.select_dtypes(include=['object', 'string']).columns

    # âœ… ìˆ«ì feature normalize
    scaler = MinMaxScaler()
    X_numeric = scaler.fit_transform(df[numeric_cols].values)

    # âœ… ë¬¸ìì—´ feature â†’ integer encoding
    X_strings = []
    for col in string_cols:
        unique_vals = np.unique(df[col])
        val_to_int = {val: idx for idx, val in enumerate(unique_vals)}
        encoded_col = np.vectorize(val_to_int.get)(df[col].values).reshape(-1, 1)
        X_strings.append(encoded_col)

    # âœ… ìˆ«ì + ë¬¸ìì—´ feature ê²°í•©
    if X_strings:
        X_strings = np.hstack(X_strings)
        X = np.hstack([X_numeric, X_strings])
    else:
        X = X_numeric

    return X, y


            
def main(args):
    # ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_dir = f"{args.dataset_name}_experiment_results_{timestamp}"
    if not os.path.exists(results_dir):
        os.makedirs(results_dir)

    # âœ… log.txtë¡œ + í„°ë¯¸ë„ ë™ì‹œì— ì¶œë ¥
    log_file = open(os.path.join(results_dir, "log.txt"), "w")
    sys.stdout = Tee(sys.stdout, log_file)
    sys.stderr = Tee(sys.stderr, log_file)
    
    # ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ
    # dataset_path = f'/lab-di/nfsdata/home/suhee.yoon/autoanoeval/ADBench/adbench/datasets/Classical/{args.dataset_name}.npz'
    # dataset_url = f"https://github.com/Minqi824/ADBench/raw/main/adbench/datasets/Classical/{args.dataset_name}.npz"
    # download_dataset(dataset_url, dataset_path)


    # ë°ì´í„°ì…‹ ë¡œë“œ
    print("ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...")
    # data = np.load(dataset_path, allow_pickle=True)
    # X_original, y_original = data['X'], data['y']
    X_original, y_original = load_dataset(args.dataset_name)

    print(f"ì›ë³¸ ë°ì´í„°ì…‹ shape: {X_original.shape}")
    print(f"ì›ë³¸ í´ë˜ìŠ¤ ë¶„í¬ - ì •ìƒ: {np.sum(y_original == 0)}, ì´ìƒ: {np.sum(y_original == 1)}")

    # ë°ì´í„°ì…‹ ì¤€ë¹„: ì •ìƒ ë°ì´í„°ì™€ ì´ìƒ ë°ì´í„° ë¶„ë¦¬
    X_normal = X_original[y_original == 0]
    X_anomaly = X_original[y_original == 1]
    
    # 1. ì •ìƒ ë°ì´í„° ë¶„í• : í•™ìŠµìš©(train)ê³¼ ê²€ì¦/í…ŒìŠ¤íŠ¸ìš©(holdout)
    X_normal_train, X_normal_holdout = train_test_split(X_normal, test_size=0.4, random_state=RANDOM_SEED)
    
    # 2. ì‹¤ì œ ì´ìƒ ë°ì´í„° ë¶„í• : ê²€ì¦ìš©(50%)ê³¼ í…ŒìŠ¤íŠ¸ìš©(50%)
    X_anomaly_val, X_anomaly_test = train_test_split(X_anomaly, test_size=0.7, random_state=RANDOM_SEED)

    # 3. ê²€ì¦ ì„¸íŠ¸ì™€ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì— ì •ìƒ ë°ì´í„° ì¶”ê°€ (holdout ë°ì´í„°ì˜ ì ˆë°˜ì”©)
    X_normal_val, X_normal_test = train_test_split(X_normal_holdout, test_size=0.5, random_state=RANDOM_SEED)
    
    # Real anomaly validation setê³¼ test set ìƒì„±
    X_val_real = np.vstack([X_normal_val, X_anomaly_val])
    y_val_real = np.concatenate([np.zeros(len(X_normal_val)), np.ones(len(X_anomaly_val))])
    
    X_test = np.vstack([X_normal_test, X_anomaly_test])
    y_test = np.concatenate([np.zeros(len(X_normal_test)), np.ones(len(X_anomaly_test))])
    
    # ê° ë°ì´í„°ì…‹ ì…”í”Œ
    def shuffle_data(X, y):
        idx = np.random.RandomState(RANDOM_SEED).permutation(len(y))
        return X[idx], y[idx]
    
    X_val_real, y_val_real = shuffle_data(X_val_real, y_val_real)
    X_test, y_test = shuffle_data(X_test, y_test)
    
    print(f"\në°ì´í„°ì…‹ ë¶„í•  ì™„ë£Œ:")
    print(f"Train set (ì •ìƒë§Œ): {X_normal_train.shape}")
    print(f"Real validation set: {X_val_real.shape}, ì •ìƒ: {np.sum(y_val_real == 0)}, ì´ìƒ: {np.sum(y_val_real == 1)}")
    print(f"Test set: {X_test.shape}, ì •ìƒ: {np.sum(y_test == 0)}, ì´ìƒ: {np.sum(y_test == 1)}")
    
    # ì—¬ëŸ¬ ìœ í˜•ì˜ Synthetic Anomaly ìƒì„±
    data_generator = SimpleDataGenerator(seed=RANDOM_SEED)
    synthetic_val_sets = {}
    
    for anomaly_type in ANOMALY_TYPES:
        print(f"\n{anomaly_type} ìœ í˜•ì˜ í•©ì„± ì´ìƒì¹˜ë¡œ ê²€ì¦ ì„¸íŠ¸ ìƒì„± ì¤‘...")
        
        # í•©ì„± ì´ìƒì¹˜ ìƒì„± (ì‹¤ì œ ì´ìƒì¹˜ì™€ ë™ì¼í•œ ê°œìˆ˜ë¡œ)
        synthetic_anomalies = data_generator.generate_anomalies(
            X=X_original,
            y=y_original,
            anomaly_type=anomaly_type,
            alpha=5,  # local, cluster ì´ìƒì¹˜ ê°•ë„
            percentage=0.2,  # global ì´ìƒì¹˜ ë²”ìœ„
            anomaly_count=len(X_anomaly_val)
        )
        
        # í•©ì„± ì´ìƒì¹˜ë¡œ ê²€ì¦ ì„¸íŠ¸ ìƒì„±
        X_val_synthetic = np.vstack([X_normal_val, synthetic_anomalies])
        y_val_synthetic = np.concatenate([np.zeros(len(X_normal_val)), np.ones(len(synthetic_anomalies))])
        X_val_synthetic, y_val_synthetic = shuffle_data(X_val_synthetic, y_val_synthetic)
        
        synthetic_val_sets[anomaly_type] = (X_val_synthetic, y_val_synthetic)
        
        print(f"{anomaly_type} ê²€ì¦ ì„¸íŠ¸: {X_val_synthetic.shape}, ì •ìƒ: {np.sum(y_val_synthetic == 0)}, ì´ìƒ: {np.sum(y_val_synthetic == 1)}")
    
    # t-SNE ì‹œê°í™”
    print("\nt-SNE ì‹œê°í™” ìƒì„± ì¤‘...")
    
    # ì‹¤ì œ ì´ìƒì¹˜ì— ëŒ€í•œ ì‹œê°í™”
    visualize_tsne(
        X_test, y_test, None,
        title='Real Anomalies t-SNE Visualization',
        filename=os.path.join(results_dir, 'real_anomalies_tsne.png'),
        anomaly_types=None
    )
    
    # ëª¨ë“  í•©ì„± ì´ìƒì¹˜ ìœ í˜•ê³¼ ì •ìƒ ë°ì´í„° ê²°í•©
    X_all_synthetic = X_normal_test.copy()
    y_all_synthetic = np.zeros(len(X_normal_test))
    y_types = np.zeros(len(X_normal_test))
    
    # ê° í•©ì„± ì´ìƒì¹˜ ìœ í˜• ì¶”ê°€
    for i, anomaly_type in enumerate(synthetic_val_sets.keys(), 1):
        X_val, y_val = synthetic_val_sets[anomaly_type]
        synthetic_anomalies = X_val[y_val == 1]
        
        X_all_synthetic = np.vstack([X_all_synthetic, synthetic_anomalies])
        y_all_synthetic = np.concatenate([y_all_synthetic, np.ones(len(synthetic_anomalies))])
        y_types = np.concatenate([y_types, np.full(len(synthetic_anomalies), i)])
    
    # í•©ì„± ì´ìƒì¹˜ ìœ í˜•ë³„ ì‹œê°í™”
    visualize_tsne(
        X_all_synthetic, y_all_synthetic, y_types,
        title='Synthetic Anomalies t-SNE Visualization',
        filename=os.path.join(results_dir, 'synthetic_anomalies_tsne.png'),
        anomaly_types=ANOMALY_TYPES
    )
    
    # PyOD ëª¨ë¸ ì„ íƒ ì‹¤í—˜ ì‹¤í–‰
    print("\nPyOD ëª¨ë¸ ì„ íƒ ì‹¤í—˜ ì‹¤í–‰ ì¤‘...")
    run_model_selection_experiment(
        X_normal_train=X_normal_train,
        X_val_real=X_val_real, y_val_real=y_val_real,
        synthetic_val_sets=synthetic_val_sets,
        X_test=X_test, y_test=y_test,
        results_dir=results_dir
    )

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--dataset_name", type=str, required=True)
    args = parser.parse_args()
    main(args)