import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os
import time
from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, average_precision_score
from sklearn.preprocessing import StandardScaler
from pyod.models.abod import ABOD
from pyod.models.knn import KNN
from pyod.models.lof import LOF
from pyod.models.cof import COF
from pyod.models.iforest import IForest
from pyod.models.ocsvm import OCSVM
from pyod.models.copod import COPOD
from pyod.models.pca import PCA
from pyod.models.hbos import HBOS
from pyod.models.mcd import MCD
from pyod.models.loda import LODA
from pyod.models.cblof import CBLOF

def run_model_selection_experiment(X_normal_train, X_val_real, y_val_real, 
                                   synthetic_val_sets, X_test, y_test, results_dir):
    """
    PyOD ëª¨ë¸ ì„ íƒ ì‹¤í—˜ ì‹¤í–‰ í•¨ìˆ˜
    
    Parameters:
    - X_normal_train: í•™ìŠµìš© ì •ìƒ ë°ì´í„°
    - X_val_real: ì‹¤ì œ ì´ìƒì¹˜ê°€ í¬í•¨ëœ ê²€ì¦ ë°ì´í„°
    - y_val_real: ì‹¤ì œ ì´ìƒì¹˜ê°€ í¬í•¨ëœ ê²€ì¦ ë°ì´í„°ì˜ ë ˆì´ë¸”
    - synthetic_val_sets: í•©ì„± ì´ìƒì¹˜ ìœ í˜•ë³„ ê²€ì¦ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
    - X_test: í…ŒìŠ¤íŠ¸ ë°ì´í„°
    - y_test: í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ë ˆì´ë¸”
    - results_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
    """
    # ë°ì´í„° í‘œì¤€í™”
    scaler = StandardScaler()
    X_normal_train_scaled = scaler.fit_transform(X_normal_train)
    X_val_real_scaled = scaler.transform(X_val_real)
    X_test_scaled = scaler.transform(X_test)
    
    # í•©ì„± ì´ìƒì¹˜ ë°ì´í„° í‘œì¤€í™”
    synthetic_val_sets_scaled = {}
    for anomaly_type, (X_val, y_val) in synthetic_val_sets.items():
        synthetic_val_sets_scaled[anomaly_type] = (scaler.transform(X_val), y_val)
    
    # PyOD ëª¨ë¸ í›„ë³´êµ° ì •ì˜
    models = {
        'ABOD': ABOD(contamination=0.1, n_neighbors=10),
        'KNN': KNN(contamination=0.1, n_neighbors=5),
        'LOF': LOF(contamination=0.1, n_neighbors=20),
        'COF': COF(contamination=0.1, n_neighbors=20),
        'IForest': IForest(contamination=0.1, random_state=42),
        'OCSVM': OCSVM(contamination=0.1, kernel='rbf'),
        'COPOD': COPOD(contamination=0.1),
        'PCA': PCA(contamination=0.1, random_state=42),
        'HBOS': HBOS(contamination=0.1),
        'MCD': MCD(contamination=0.1, random_state=42),
        'LODA': LODA(contamination=0.1), 
        'CBLOF': CBLOF(contamination=0.1, random_state=42),
    }
    
    # ê²°ê³¼ ì €ì¥ ë”•ì…”ë„ˆë¦¬
    results = {}
    
    # 1. ì‹¤ì œ ì´ìƒì¹˜ë¡œ ê²€ì¦í•˜ëŠ” ê²½ìš°ì˜ ì‹¤í—˜
    print("\n1. ì‹¤ì œ ì´ìƒì¹˜ë¡œ ê²€ì¦í•˜ëŠ” ê²½ìš°ì˜ ì‹¤í—˜ ì‹¤í–‰ ì¤‘...")
    real_val_results = evaluate_models(
        models=models,
        X_train=X_normal_train_scaled,
        X_val=X_val_real_scaled,
        y_val=y_val_real,
        X_test=X_test_scaled,
        y_test=y_test
    )
    
    results['real_validation'] = real_val_results
    
    # 2. ê° ìœ í˜•ì˜ í•©ì„± ì´ìƒì¹˜ë¡œ ê²€ì¦í•˜ëŠ” ê²½ìš°ì˜ ì‹¤í—˜
    for anomaly_type, (X_val_synthetic_scaled, y_val_synthetic) in synthetic_val_sets_scaled.items():
        print(f"\n2. {anomaly_type} í•©ì„± ì´ìƒì¹˜ë¡œ ê²€ì¦í•˜ëŠ” ê²½ìš°ì˜ ì‹¤í—˜ ì‹¤í–‰ ì¤‘...")
        synthetic_val_results = evaluate_models(
            models=models,
            X_train=X_normal_train_scaled,
            X_val=X_val_synthetic_scaled,
            y_val=y_val_synthetic,
            X_test=X_test_scaled,
            y_test=y_test
        )
        
        results[f'synthetic_{anomaly_type}_validation'] = synthetic_val_results
    
    # ê²°ê³¼ ìš”ì•½ ë° ì‹œê°í™”
    print("\nê²°ê³¼ ìš”ì•½ ìƒì„± ì¤‘...")
    summarize_results(results, results_dir)
    
    return results

def evaluate_models(models, X_train, X_val, y_val, X_test, y_test):
    """
    ì—¬ëŸ¬ ëª¨ë¸ì„ í•™ìŠµ, ê²€ì¦, í…ŒìŠ¤íŠ¸í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
    
    Parameters:
    - models: PyOD ëª¨ë¸ ë”•ì…”ë„ˆë¦¬
    - X_train: í•™ìŠµ ë°ì´í„° (ì •ìƒë§Œ)
    - X_val: ê²€ì¦ ë°ì´í„°
    - y_val: ê²€ì¦ ë°ì´í„°ì˜ ë ˆì´ë¸”
    - X_test: í…ŒìŠ¤íŠ¸ ë°ì´í„°
    - y_test: í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ë ˆì´ë¸”
    
    Returns:
    - results: ëª¨ë¸ë³„ ì„±ëŠ¥ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    results = {}
    
    for model_name, model in models.items():
        print(f"\n{model_name} ëª¨ë¸ í‰ê°€ ì¤‘...")
        
        # ì‹œê°„ ì¸¡ì • ì‹œì‘
        start_time = time.time()
        
        try:
            # ëª¨ë¸ í•™ìŠµ (ì •ìƒ ë°ì´í„°ë§Œ ì‚¬ìš©)
            model.fit(X_train)
            
            # ê²€ì¦ ì„¸íŠ¸ì—ì„œ ì´ìƒì¹˜ ì ìˆ˜ ê³„ì‚°
            val_scores = model.decision_function(X_val)
            val_auc = roc_auc_score(y_val, val_scores)
            val_ap = average_precision_score(y_val, val_scores)
            
            # í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ ì´ìƒì¹˜ ì ìˆ˜ ê³„ì‚°
            test_scores = model.decision_function(X_test)
            test_auc = roc_auc_score(y_test, test_scores)
            test_ap = average_precision_score(y_test, test_scores)
            
            # ì‹œê°„ ì¸¡ì • ì¢…ë£Œ
            training_time = time.time() - start_time
            
            # ê²°ê³¼ ì €ì¥
            results[model_name] = {
                'val_auc': val_auc,
                'val_ap': val_ap,
                'test_auc': test_auc,
                'test_ap': test_ap,
                'training_time': training_time
            }
            
            print(f"{model_name} - Val AUC: {val_auc:.4f}, Val AP: {val_ap:.4f}, Test AUC: {test_auc:.4f}, Test AP: {test_ap:.4f}, Time: {training_time:.2f}s")
            
        except Exception as e:
            print(f"{model_name} ëª¨ë¸ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            results[model_name] = {
                'val_auc': float('nan'),
                'val_ap': float('nan'),
                'test_auc': float('nan'),
                'test_ap': float('nan'),
                'training_time': float('nan')
            }
    
    return results

def summarize_results(results, results_dir):
    """
    ëª¨ë¸ ì„ íƒ ì‹¤í—˜ ê²°ê³¼ë¥¼ ìš”ì•½í•˜ê³  ì‹œê°í™”í•˜ëŠ” í•¨ìˆ˜ (Top-3 í¬í•¨)
    
    Parameters:
    - results: ê²€ì¦ ë°©ì‹ë³„ ëª¨ë¸ ì„±ëŠ¥ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    - results_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
    """
    # ê²°ê³¼ë¥¼ ì €ì¥í•  DataFrame ìƒì„±
    summary_data = []
    
    # ê° ê²€ì¦ ë°©ì‹ë³„ top-3 ëª¨ë¸ ì„ íƒ
    top_models = {}
    
    for validation_type, val_results in results.items():
        # ê²€ì¦ ì„¸íŠ¸ AUC ê¸°ì¤€ìœ¼ë¡œ ëª¨ë¸ ì •ë ¬
        sorted_models = {k: v for k, v in sorted(
            val_results.items(), 
            key=lambda item: item[1]['val_auc'] if not np.isnan(item[1]['val_auc']) else -float('inf'), 
            reverse=True
        )}
        
        # Top-3 ëª¨ë¸ ì„ íƒ (ê²€ì¦ AUC ê¸°ì¤€)
        top_3_models = {}
        valid_models = [(k, v) for k, v in sorted_models.items() if not np.isnan(v['val_auc'])]
        
        for i, (model_name, metrics) in enumerate(valid_models[:3]):  # top-3ë§Œ ì„ íƒ
            rank = i + 1
            top_3_models[f'rank_{rank}'] = {
                'model_name': model_name,
                'val_auc': metrics['val_auc'],
                'test_auc': metrics['test_auc'],
                'val_ap': metrics['val_ap'],
                'test_ap': metrics['test_ap'],
                'training_time': metrics['training_time']
            }
        
        top_models[validation_type] = top_3_models
        
        # Top-3 ëª¨ë¸ ì •ë³´ ì¶œë ¥
        print(f"\n{validation_type} ê²€ì¦ ë°©ì‹ì˜ Top-3 ëª¨ë¸:")
        for rank, model_info in top_3_models.items():
            print(f"  {rank}: {model_info['model_name']} - Val AUC: {model_info['val_auc']:.4f}, Test AUC: {model_info['test_auc']:.4f}")
        
        # ëª¨ë“  ëª¨ë¸ì˜ ê²°ê³¼ë¥¼ DataFrameì— ì¶”ê°€ (ë­í‚¹ ì •ë³´ í¬í•¨)
        for model_name, metrics in val_results.items():
            # ë­í‚¹ ì •ë³´ ì¶”ê°€
            rank = None
            for rank_key, model_info in top_3_models.items():
                if model_info['model_name'] == model_name:
                    rank = int(rank_key.split('_')[1])
                    break
            
            row = {
                'validation_type': validation_type,
                'model': model_name,
                'rank': rank,  # Top-3ì— ë“¤ì§€ ì•Šìœ¼ë©´ None
                'val_auc': metrics['val_auc'],
                'test_auc': metrics['test_auc'],
                'val_ap': metrics['val_ap'],
                'test_ap': metrics['test_ap'],
                'training_time': metrics['training_time']
            }
            summary_data.append(row)
    
    # DataFrame ìƒì„± ë° CSVë¡œ ì €ì¥
    summary_df = pd.DataFrame(summary_data)
    csv_filename = os.path.join(results_dir, 'model_selection_results.csv')
    summary_df.to_csv(csv_filename, index=False)
    print(f"\nëª¨ë¸ ì„ íƒ ê²°ê³¼ê°€ {csv_filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤")
    
    # Top-3 ëª¨ë¸ ì •ë³´ë§Œ ë³„ë„ë¡œ ì €ì¥
    save_top3_results(top_models, results_dir)
    
    # Top-3 ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ì‹œê°í™”
    plot_top_models_comparison(top_models, results_dir)
    
    # Top-3 ëª¨ë¸ ì„±ëŠ¥ íˆíŠ¸ë§µ ìƒì„±
    plot_top3_performance_heatmap(top_models, results_dir)
    
    # ê²€ì¦-í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ìƒê´€ê´€ê³„ ë¶„ì„
    analyze_validation_test_correlation(summary_df, results_dir)
    
    # Top-3 ë¶„ì„ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
    generate_top3_summary_report(top_models, results_dir)
    
    return top_models

def save_top3_results(top_models, results_dir):
    """
    Top-3 ëª¨ë¸ ê²°ê³¼ë¥¼ ë³„ë„ CSVë¡œ ì €ì¥
    
    Parameters:
    - top_models: ê²€ì¦ ë°©ì‹ë³„ top-3 ëª¨ë¸ ì •ë³´ ë”•ì…”ë„ˆë¦¬
    - results_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
    """
    top3_data = []
    
    for validation_type, ranks in top_models.items():
        for rank_key, model_info in ranks.items():
            rank = int(rank_key.split('_')[1])
            row = {
                'validation_type': validation_type,
                'rank': rank,
                'model_name': model_info['model_name'],
                'val_auc': model_info['val_auc'],
                'test_auc': model_info['test_auc'],
                'val_ap': model_info['val_ap'],
                'test_ap': model_info['test_ap'],
                'training_time': model_info['training_time']
            }
            top3_data.append(row)
    
    top3_df = pd.DataFrame(top3_data)
    top3_csv = os.path.join(results_dir, 'top3_models_results.csv')
    top3_df.to_csv(top3_csv, index=False)
    print(f"Top-3 ëª¨ë¸ ê²°ê³¼ê°€ {top3_csv}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤")

def plot_top_models_comparison(top_models, results_dir):
    """
    ê° ê²€ì¦ ë°©ì‹ë³„ top-3 ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë¹„êµí•˜ëŠ” ì‹œê°í™”
    
    Parameters:
    - top_models: ê²€ì¦ ë°©ì‹ë³„ top-3 ëª¨ë¸ ì •ë³´ ë”•ì…”ë„ˆë¦¬
    - results_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
    """
    validation_types = list(top_models.keys())
    
    # í‘œì‹œìš© ë ˆì´ë¸” ë³€í™˜
    display_labels = []
    for vtype in validation_types:
        if vtype == 'real_validation':
            display_labels.append('Real Anomaly')
        elif vtype.startswith('synthetic_'):
            anomaly_type = vtype.replace('synthetic_', '').replace('_validation', '')
            display_labels.append(f'Synthetic {anomaly_type.capitalize()}')
    
    # AUC ë¹„êµ ê·¸ë˜í”„
    plt.figure(figsize=(16, 10))
    
    x = np.arange(len(display_labels))
    width = 0.25  # 3ê°œ ëª¨ë¸ì„ í‘œì‹œí•˜ê¸° ìœ„í•´ ì¢ê²Œ ì„¤ì •
    
    # ê° ë­í¬ë³„ ë°ì´í„° ìˆ˜ì§‘
    rank1_val_aucs = []
    rank1_test_aucs = []
    rank1_models = []
    
    rank2_val_aucs = []
    rank2_test_aucs = []
    rank2_models = []
    
    rank3_val_aucs = []
    rank3_test_aucs = []
    rank3_models = []
    
    for validation_type in validation_types:
        ranks = top_models[validation_type]
        
        # Rank 1
        if 'rank_1' in ranks:
            rank1_val_aucs.append(ranks['rank_1']['val_auc'])
            rank1_test_aucs.append(ranks['rank_1']['test_auc'])
            rank1_models.append(ranks['rank_1']['model_name'])
        else:
            rank1_val_aucs.append(0)
            rank1_test_aucs.append(0)
            rank1_models.append('')
        
        # Rank 2
        if 'rank_2' in ranks:
            rank2_val_aucs.append(ranks['rank_2']['val_auc'])
            rank2_test_aucs.append(ranks['rank_2']['test_auc'])
            rank2_models.append(ranks['rank_2']['model_name'])
        else:
            rank2_val_aucs.append(0)
            rank2_test_aucs.append(0)
            rank2_models.append('')
        
        # Rank 3
        if 'rank_3' in ranks:
            rank3_val_aucs.append(ranks['rank_3']['val_auc'])
            rank3_test_aucs.append(ranks['rank_3']['test_auc'])
            rank3_models.append(ranks['rank_3']['model_name'])
        else:
            rank3_val_aucs.append(0)
            rank3_test_aucs.append(0)
            rank3_models.append('')
    
    # ê²€ì¦ AUC ë°” ê·¸ë˜í”„
    bars1_1 = plt.bar(x - width, rank1_val_aucs, width, label='Rank 1 (Val)', alpha=0.8, color='darkblue')
    bars1_2 = plt.bar(x, rank2_val_aucs, width, label='Rank 2 (Val)', alpha=0.8, color='blue')
    bars1_3 = plt.bar(x + width, rank3_val_aucs, width, label='Rank 3 (Val)', alpha=0.8, color='lightblue')
    
    plt.xlabel('Validation Method', fontsize=12)
    plt.ylabel('Validation AUC Score', fontsize=12)
    plt.title('Top-3 Models Validation AUC Comparison by Validation Method', fontsize=16)
    plt.xticks(x, display_labels, rotation=20, ha='right')
    plt.ylim(0.5, 1.0)
    plt.legend()
    
    # ëª¨ë¸ ì´ë¦„ í‘œì‹œ
    for i, (bar1, bar2, bar3) in enumerate(zip(bars1_1, bars1_2, bars1_3)):
        if rank1_models[i]:
            plt.text(bar1.get_x() + bar1.get_width()/2, 0.52,
                    rank1_models[i], ha='center', va='bottom', rotation=90, fontsize=9)
        if rank2_models[i]:
            plt.text(bar2.get_x() + bar2.get_width()/2, 0.52,
                    rank2_models[i], ha='center', va='bottom', rotation=90, fontsize=9)
        if rank3_models[i]:
            plt.text(bar3.get_x() + bar3.get_width()/2, 0.52,
                    rank3_models[i], ha='center', va='bottom', rotation=90, fontsize=9)
    
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.tight_layout()
    
    # ì €ì¥
    val_auc_filename = os.path.join(results_dir, 'top3_models_validation_auc_comparison.png')
    plt.savefig(val_auc_filename, dpi=300)
    plt.close()
    print(f"Top-3 ê²€ì¦ AUC ë¹„êµ ê·¸ë˜í”„ê°€ {val_auc_filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤")
    
    # í…ŒìŠ¤íŠ¸ AUC ë¹„êµ ê·¸ë˜í”„
    plt.figure(figsize=(16, 10))
    
    bars2_1 = plt.bar(x - width, rank1_test_aucs, width, label='Rank 1 (Test)', alpha=0.8, color='darkred')
    bars2_2 = plt.bar(x, rank2_test_aucs, width, label='Rank 2 (Test)', alpha=0.8, color='red')
    bars2_3 = plt.bar(x + width, rank3_test_aucs, width, label='Rank 3 (Test)', alpha=0.8, color='lightcoral')
    
    plt.xlabel('Validation Method', fontsize=12)
    plt.ylabel('Test AUC Score', fontsize=12)
    plt.title('Top-3 Models Test AUC Comparison by Validation Method', fontsize=16)
    plt.xticks(x, display_labels, rotation=20, ha='right')
    plt.ylim(0.5, 1.0)
    plt.legend()
    
    # ëª¨ë¸ ì´ë¦„ í‘œì‹œ
    for i, (bar1, bar2, bar3) in enumerate(zip(bars2_1, bars2_2, bars2_3)):
        if rank1_models[i]:
            plt.text(bar1.get_x() + bar1.get_width()/2, 0.52,
                    rank1_models[i], ha='center', va='bottom', rotation=90, fontsize=9)
        if rank2_models[i]:
            plt.text(bar2.get_x() + bar2.get_width()/2, 0.52,
                    rank2_models[i], ha='center', va='bottom', rotation=90, fontsize=9)
        if rank3_models[i]:
            plt.text(bar3.get_x() + bar3.get_width()/2, 0.52,
                    rank3_models[i], ha='center', va='bottom', rotation=90, fontsize=9)
    
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.tight_layout()
    
    # ì €ì¥
    test_auc_filename = os.path.join(results_dir, 'top3_models_test_auc_comparison.png')
    plt.savefig(test_auc_filename, dpi=300)
    plt.close()
    print(f"Top-3 í…ŒìŠ¤íŠ¸ AUC ë¹„êµ ê·¸ë˜í”„ê°€ {test_auc_filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤")
    
    # Combined AUC ë¹„êµ ê·¸ë˜í”„ (ê²€ì¦ vs í…ŒìŠ¤íŠ¸)
    plot_combined_top3_comparison(top_models, results_dir)

def plot_combined_top3_comparison(top_models, results_dir):
    """
    Top-3 ëª¨ë¸ì˜ ê²€ì¦ vs í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ì„ í•¨ê»˜ ë¹„êµí•˜ëŠ” ê·¸ë˜í”„
    
    Parameters:
    - top_models: ê²€ì¦ ë°©ì‹ë³„ top-3 ëª¨ë¸ ì •ë³´ ë”•ì…”ë„ˆë¦¬
    - results_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
    """
    validation_types = list(top_models.keys())
    
    # í‘œì‹œìš© ë ˆì´ë¸” ë³€í™˜
    display_labels = []
    for vtype in validation_types:
        if vtype == 'real_validation':
            display_labels.append('Real Anomaly')
        elif vtype.startswith('synthetic_'):
            anomaly_type = vtype.replace('synthetic_', '').replace('_validation', '')
            display_labels.append(f'Synthetic {anomaly_type.capitalize()}')
    
    # ì„œë¸Œí”Œë¡¯ ìƒì„± (ê° ê²€ì¦ ë°©ì‹ë§ˆë‹¤ í•˜ë‚˜ì”©)
    fig, axes = plt.subplots(1, len(validation_types), figsize=(5*len(validation_types), 6))
    if len(validation_types) == 1:
        axes = [axes]
    
    for i, (validation_type, display_label) in enumerate(zip(validation_types, display_labels)):
        ax = axes[i]
        ranks = top_models[validation_type]
        
        # ë°ì´í„° ì¤€ë¹„
        models = []
        val_aucs = []
        test_aucs = []
        
        for rank_key in ['rank_1', 'rank_2', 'rank_3']:
            if rank_key in ranks:
                models.append(ranks[rank_key]['model_name'])
                val_aucs.append(ranks[rank_key]['val_auc'])
                test_aucs.append(ranks[rank_key]['test_auc'])
        
        if models:  # ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
            x = np.arange(len(models))
            width = 0.35
            
            bars1 = ax.bar(x - width/2, val_aucs, width, label='Validation AUC', alpha=0.8)
            bars2 = ax.bar(x + width/2, test_aucs, width, label='Test AUC', alpha=0.8)
            
            ax.set_xlabel('Model Rank', fontsize=10)
            ax.set_ylabel('AUC Score', fontsize=10)
            ax.set_title(f'{display_label}', fontsize=12)
            ax.set_xticks(x)
            ax.set_xticklabels([f'Rank {j+1}\n{model}' for j, model in enumerate(models)], 
                              rotation=45, ha='right', fontsize=9)
            ax.set_ylim(0.5, 1.0)
            ax.legend(fontsize=9)
            ax.grid(True, linestyle='--', alpha=0.7)
    
    plt.tight_layout()
    
    # ì €ì¥
    combined_filename = os.path.join(results_dir, 'top3_models_combined_comparison.png')
    plt.savefig(combined_filename, dpi=300)
    plt.close()
    print(f"Top-3 í†µí•© ë¹„êµ ê·¸ë˜í”„ê°€ {combined_filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤")

def analyze_validation_test_correlation(summary_df, results_dir):
    """
    ê²€ì¦ ì„±ëŠ¥ê³¼ í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ ë¶„ì„í•˜ëŠ” í•¨ìˆ˜
    
    Parameters:
    - summary_df: ì‹¤í—˜ ê²°ê³¼ ìš”ì•½ DataFrame
    - results_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
    """
    validation_types = summary_df['validation_type'].unique()
    
    # ê° ê²€ì¦ ë°©ì‹ë³„ë¡œ ì‚°ì ë„ ìƒì„±
    plt.figure(figsize=(15, 10))
    
    for i, val_type in enumerate(validation_types):
        df_subset = summary_df[summary_df['validation_type'] == val_type]
        
        # NaN ê°’ ì œì™¸
        df_subset = df_subset.dropna(subset=['val_auc', 'test_auc'])
        
        if len(df_subset) > 0:
            # ìƒ‰ìƒ ë° ë§ˆì»¤ ì„¤ì •
            colors = ['blue', 'red', 'green', 'purple', 'orange', 'cyan']
            markers = ['o', '^', 's', 'D', '*', 'X']
            
            color = colors[i % len(colors)]
            marker = markers[i % len(markers)]
            
            # ë ˆì´ë¸” ë³€í™˜
            if val_type == 'real_validation':
                label = 'Real Anomaly'
            elif val_type.startswith('synthetic_'):
                anomaly_type = val_type.replace('synthetic_', '').replace('_validation', '')
                label = f'Synthetic {anomaly_type.capitalize()}'
            else:
                label = val_type
            
            # Top-3 ëª¨ë¸ ê°•ì¡° í‘œì‹œ
            top3_subset = df_subset[df_subset['rank'].notna()]
            other_subset = df_subset[df_subset['rank'].isna()]
            
            # Top-3 ëª¨ë¸
            if len(top3_subset) > 0:
                plt.scatter(
                    top3_subset['val_auc'],
                    top3_subset['test_auc'],
                    alpha=0.9,
                    label=f'{label} (Top-3)',
                    color=color,
                    marker=marker,
                    s=150,
                    edgecolors='black',
                    linewidth=2
                )
                
                # Top-3 ëª¨ë¸ ì´ë¦„ í‘œì‹œ
                for _, row in top3_subset.iterrows():
                    plt.annotate(
                        f"{row['model']} (R{int(row['rank'])})",
                        (row['val_auc'], row['test_auc']),
                        fontsize=8,
                        xytext=(5, 5),
                        textcoords='offset points',
                        fontweight='bold'
                    )
            
            # ë‚˜ë¨¸ì§€ ëª¨ë¸ë“¤
            if len(other_subset) > 0:
                plt.scatter(
                    other_subset['val_auc'],
                    other_subset['test_auc'],
                    alpha=0.4,
                    color=color,
                    marker=marker,
                    s=50
                )
    
    # ëŒ€ê°ì„  ê·¸ë¦¬ê¸° (x=y)
    min_val = min(summary_df['val_auc'].min(), summary_df['test_auc'].min())
    max_val = max(summary_df['val_auc'].max(), summary_df['test_auc'].max())
    plt.plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.5)
    
    plt.xlabel('Validation AUC', fontsize=12)
    plt.ylabel('Test AUC', fontsize=12)
    plt.title('Correlation between Validation and Test Performance (Top-3 Highlighted)', fontsize=16)
    plt.legend(fontsize=10)
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.axis('equal')
    plt.tight_layout()
    
    # ì €ì¥
    corr_filename = os.path.join(results_dir, 'validation_test_correlation_top3.png')
    plt.savefig(corr_filename, dpi=300)
    plt.close()
    print(f"ê²€ì¦-í…ŒìŠ¤íŠ¸ ìƒê´€ê´€ê³„ ê·¸ë˜í”„ (Top-3 ê°•ì¡°)ê°€ {corr_filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤")
    
    # ì¶”ê°€: ê° ê²€ì¦ ë°©ì‹ë³„ Spearman ìƒê´€ê³„ìˆ˜ ê³„ì‚°
    correlation_results = []
    
    for val_type in validation_types:
        df_subset = summary_df[summary_df['validation_type'] == val_type].dropna(subset=['val_auc', 'test_auc'])
        
        if len(df_subset) > 1:  # ìƒê´€ê³„ìˆ˜ë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ ìµœì†Œ 2ê°œ ì´ìƒì˜ ë°ì´í„° í•„ìš”
            corr_auc = df_subset[['val_auc', 'test_auc']].corr(method='spearman').iloc[0, 1]
            corr_ap = df_subset[['val_ap', 'test_ap']].corr(method='spearman').iloc[0, 1]
            
            correlation_results.append({
                'validation_type': val_type,
                'spearman_corr_auc': corr_auc,
                'spearman_corr_ap': corr_ap
            })
    
    # ìƒê´€ê³„ìˆ˜ ê²°ê³¼ ì €ì¥
    if correlation_results:
        corr_df = pd.DataFrame(correlation_results)
        corr_csv = os.path.join(results_dir, 'validation_test_correlation_coefficients.csv')
        corr_df.to_csv(corr_csv, index=False)
        print(f"ìƒê´€ê³„ìˆ˜ ê²°ê³¼ê°€ {corr_csv}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤")
        
        # ìƒê´€ê³„ìˆ˜ ì‹œê°í™”
        plt.figure(figsize=(12, 6))
        
        x = np.arange(len(corr_df))
        width = 0.35
        
        # í‘œì‹œìš© ë ˆì´ë¸” ë³€í™˜
        display_labels = []
        for vtype in corr_df['validation_type']:
            if vtype == 'real_validation':
                display_labels.append('Real Anomaly')
            elif vtype.startswith('synthetic_'):
                anomaly_type = vtype.replace('synthetic_', '').replace('_validation', '')
                display_labels.append(f'Synthetic {anomaly_type.capitalize()}')
        
        plt.bar(x - width/2, corr_df['spearman_corr_auc'], width, label='AUC Correlation')
        plt.bar(x + width/2, corr_df['spearman_corr_ap'], width, label='AP Correlation')
        
        plt.xlabel('Validation Method', fontsize=12)
        plt.ylabel('Spearman Correlation Coefficient', fontsize=12)
        plt.title('Correlation between Validation and Test Performance Metrics', fontsize=16)
        plt.xticks(x, display_labels, rotation=20, ha='right')
        plt.axhline(y=0, color='k', linestyle='--', alpha=0.5)
        plt.ylim(-1.1, 1.1)
        plt.legend()
        plt.grid(True, linestyle='--', alpha=0.7)
        plt.tight_layout()
        
        corr_bar_filename = os.path.join(results_dir, 'correlation_coefficients_comparison.png')
        plt.savefig(corr_bar_filename, dpi=300)
        plt.close()
        print(f"ìƒê´€ê³„ìˆ˜ ë¹„êµ ê·¸ë˜í”„ê°€ {corr_bar_filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤")

def plot_top3_performance_heatmap(top_models, results_dir):
    """
    Top-3 ëª¨ë¸ë“¤ì˜ ì„±ëŠ¥ì„ íˆíŠ¸ë§µìœ¼ë¡œ ì‹œê°í™”
    
    Parameters:
    - top_models: ê²€ì¦ ë°©ì‹ë³„ top-3 ëª¨ë¸ ì •ë³´ ë”•ì…”ë„ˆë¦¬
    - results_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
    """
    # ë°ì´í„° ì¤€ë¹„
    validation_methods = []
    model_names = []
    val_aucs = []
    test_aucs = []
    ranks = []
    
    for validation_type, ranks_dict in top_models.items():
        # í‘œì‹œìš© ë ˆì´ë¸” ë³€í™˜
        if validation_type == 'real_validation':
            display_label = 'Real Anomaly'
        elif validation_type.startswith('synthetic_'):
            anomaly_type = validation_type.replace('synthetic_', '').replace('_validation', '')
            display_label = f'Synthetic {anomaly_type.capitalize()}'
        else:
            display_label = validation_type
        
        for rank_key, model_info in ranks_dict.items():
            rank = int(rank_key.split('_')[1])
            validation_methods.append(display_label)
            model_names.append(f"{model_info['model_name']} (R{rank})")
            val_aucs.append(model_info['val_auc'])
            test_aucs.append(model_info['test_auc'])
            ranks.append(rank)
    
    # DataFrame ìƒì„±
    heatmap_df = pd.DataFrame({
        'Validation_Method': validation_methods,
        'Model': model_names,
        'Validation_AUC': val_aucs,
        'Test_AUC': test_aucs,
        'Rank': ranks
    })
    
    # í”¼ë²— í…Œì´ë¸” ìƒì„± (ê²€ì¦ AUC)
    pivot_val = heatmap_df.pivot_table(
        index='Model', 
        columns='Validation_Method', 
        values='Validation_AUC', 
        aggfunc='mean'
    )
    
    # í”¼ë²— í…Œì´ë¸” ìƒì„± (í…ŒìŠ¤íŠ¸ AUC)
    pivot_test = heatmap_df.pivot_table(
        index='Model', 
        columns='Validation_Method', 
        values='Test_AUC', 
        aggfunc='mean'
    )
    
    # íˆíŠ¸ë§µ ê·¸ë¦¬ê¸°
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 12))
    
    # ê²€ì¦ AUC íˆíŠ¸ë§µ
    im1 = ax1.imshow(pivot_val.values, cmap='YlOrRd', aspect='auto', vmin=0.5, vmax=1.0)
    ax1.set_xticks(range(len(pivot_val.columns)))
    ax1.set_yticks(range(len(pivot_val.index)))
    ax1.set_xticklabels(pivot_val.columns, rotation=45, ha='right')
    ax1.set_yticklabels(pivot_val.index)
    ax1.set_title('Top-3 Models Validation AUC Heatmap', fontsize=14, pad=20)
    
    # ê°’ í‘œì‹œ
    for i in range(len(pivot_val.index)):
        for j in range(len(pivot_val.columns)):
            if not np.isnan(pivot_val.iloc[i, j]):
                text = ax1.text(j, i, f'{pivot_val.iloc[i, j]:.3f}',
                               ha="center", va="center", color="black", fontsize=10)
    
    # ì»¬ëŸ¬ë°”
    cbar1 = plt.colorbar(im1, ax=ax1, shrink=0.8)
    cbar1.set_label('Validation AUC', rotation=270, labelpad=15)
    
    # í…ŒìŠ¤íŠ¸ AUC íˆíŠ¸ë§µ
    im2 = ax2.imshow(pivot_test.values, cmap='YlOrRd', aspect='auto', vmin=0.5, vmax=1.0)
    ax2.set_xticks(range(len(pivot_test.columns)))
    ax2.set_yticks(range(len(pivot_test.index)))
    ax2.set_xticklabels(pivot_test.columns, rotation=45, ha='right')
    ax2.set_yticklabels(pivot_test.index)
    ax2.set_title('Top-3 Models Test AUC Heatmap', fontsize=14, pad=20)
    
    # ê°’ í‘œì‹œ
    for i in range(len(pivot_test.index)):
        for j in range(len(pivot_test.columns)):
            if not np.isnan(pivot_test.iloc[i, j]):
                text = ax2.text(j, i, f'{pivot_test.iloc[i, j]:.3f}',
                               ha="center", va="center", color="black", fontsize=10)
    
    # ì»¬ëŸ¬ë°”
    cbar2 = plt.colorbar(im2, ax=ax2, shrink=0.8)
    cbar2.set_label('Test AUC', rotation=270, labelpad=15)
    
    plt.tight_layout()
    
    # ì €ì¥
    heatmap_filename = os.path.join(results_dir, 'top3_models_performance_heatmap.png')
    plt.savefig(heatmap_filename, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"Top-3 ëª¨ë¸ ì„±ëŠ¥ íˆíŠ¸ë§µì´ {heatmap_filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤")

def generate_top3_summary_report(top_models, results_dir):
    """
    Top-3 ëª¨ë¸ ë¶„ì„ì— ëŒ€í•œ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
    
    Parameters:
    - top_models: ê²€ì¦ ë°©ì‹ë³„ top-3 ëª¨ë¸ ì •ë³´ ë”•ì…”ë„ˆë¦¬
    - results_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
    """
    report_lines = []
    report_lines.append("=" * 80)
    report_lines.append("TOP-3 MODEL SELECTION ANALYSIS REPORT")
    report_lines.append("=" * 80)
    report_lines.append("")
    
    # ì „ì²´ ìš”ì•½
    report_lines.append("ğŸ“Š OVERALL SUMMARY")
    report_lines.append("-" * 50)
    
    all_models = set()
    validation_methods = list(top_models.keys())
    
    for validation_type, ranks in top_models.items():
        for rank_key, model_info in ranks.items():
            all_models.add(model_info['model_name'])
    
    report_lines.append(f"â€¢ Total validation methods tested: {len(validation_methods)}")
    report_lines.append(f"â€¢ Unique models appearing in top-3: {len(all_models)}")
    report_lines.append(f"â€¢ Models tested: {', '.join(sorted(all_models))}")
    report_lines.append("")
    
    # ê° ê²€ì¦ ë°©ì‹ë³„ ìƒì„¸ ê²°ê³¼
    for validation_type, ranks in top_models.items():
        # ë ˆì´ë¸” ë³€í™˜
        if validation_type == 'real_validation':
            display_label = 'ğŸ¯ Real Anomaly Validation'
        elif validation_type.startswith('synthetic_'):
            anomaly_type = validation_type.replace('synthetic_', '').replace('_validation', '')
            display_label = f'ğŸ”§ Synthetic {anomaly_type.capitalize()} Validation'
        else:
            display_label = validation_type
        
        report_lines.append(display_label)
        report_lines.append("-" * len(display_label))
        
        for i in range(1, 4):  # Rank 1, 2, 3
            rank_key = f'rank_{i}'
            if rank_key in ranks:
                model_info = ranks[rank_key]
                report_lines.append(f"  Rank {i}: {model_info['model_name']}")
                report_lines.append(f"    â€¢ Validation AUC: {model_info['val_auc']:.4f}")
                report_lines.append(f"    â€¢ Test AUC: {model_info['test_auc']:.4f}")
                report_lines.append(f"    â€¢ Validation AP: {model_info['val_ap']:.4f}")
                report_lines.append(f"    â€¢ Test AP: {model_info['test_ap']:.4f}")
                report_lines.append(f"    â€¢ Training Time: {model_info['training_time']:.2f}s")
                
                # ì„±ëŠ¥ ì¼ì¹˜ë„ ë¶„ì„
                val_test_diff = abs(model_info['val_auc'] - model_info['test_auc'])
                if val_test_diff < 0.05:
                    consistency = "ğŸŸ¢ Excellent consistency"
                elif val_test_diff < 0.10:
                    consistency = "ğŸŸ¡ Good consistency"
                else:
                    consistency = "ğŸ”´ Poor consistency"
                
                report_lines.append(f"    â€¢ Val-Test Consistency: {consistency} (diff: {val_test_diff:.4f})")
                report_lines.append("")
        
        report_lines.append("")
    
    # ëª¨ë¸ë³„ ë“±ì¥ ë¹ˆë„ ë¶„ì„
    report_lines.append("ğŸ† MODEL RANKING FREQUENCY ANALYSIS")
    report_lines.append("-" * 50)
    
    model_appearances = {}
    model_rank_positions = {}
    
    for validation_type, ranks in top_models.items():
        for rank_key, model_info in ranks.items():
            model_name = model_info['model_name']
            rank = int(rank_key.split('_')[1])
            
            if model_name not in model_appearances:
                model_appearances[model_name] = 0
                model_rank_positions[model_name] = []
            
            model_appearances[model_name] += 1
            model_rank_positions[model_name].append(rank)
    
    # ë“±ì¥ ë¹ˆë„ë³„ ì •ë ¬
    sorted_models = sorted(model_appearances.items(), key=lambda x: x[1], reverse=True)
    
    for model_name, count in sorted_models:
        avg_rank = np.mean(model_rank_positions[model_name])
        rank_distribution = {1: 0, 2: 0, 3: 0}
        for rank in model_rank_positions[model_name]:
            rank_distribution[rank] += 1
        
        report_lines.append(f"â€¢ {model_name}")
        report_lines.append(f"  - Appearances in top-3: {count}/{len(validation_methods)} ({count/len(validation_methods)*100:.1f}%)")
        report_lines.append(f"  - Average rank: {avg_rank:.2f}")
        report_lines.append(f"  - Rank distribution: R1:{rank_distribution[1]}, R2:{rank_distribution[2]}, R3:{rank_distribution[3]}")
        report_lines.append("")
    
    # ê²€ì¦ ë°©ì‹ë³„ ì„±ëŠ¥ ì¼ì¹˜ë„ ë¶„ì„
    report_lines.append("ğŸ“ˆ VALIDATION-TEST CONSISTENCY ANALYSIS")
    report_lines.append("-" * 50)
    
    for validation_type, ranks in top_models.items():
        if validation_type == 'real_validation':
            display_label = 'Real Anomaly'
        elif validation_type.startswith('synthetic_'):
            anomaly_type = validation_type.replace('synthetic_', '').replace('_validation', '')
            display_label = f'Synthetic {anomaly_type.capitalize()}'
        else:
            display_label = validation_type
        
        differences = []
        for rank_key, model_info in ranks.items():
            diff = abs(model_info['val_auc'] - model_info['test_auc'])
            differences.append(diff)
        
        if differences:
            avg_diff = np.mean(differences)
            max_diff = np.max(differences)
            min_diff = np.min(differences)
            
            report_lines.append(f"â€¢ {display_label}")
            report_lines.append(f"  - Average val-test AUC difference: {avg_diff:.4f}")
            report_lines.append(f"  - Maximum difference: {max_diff:.4f}")
            report_lines.append(f"  - Minimum difference: {min_diff:.4f}")
            
            if avg_diff < 0.05:
                assessment = "ğŸŸ¢ Excellent reliability"
            elif avg_diff < 0.10:
                assessment = "ğŸŸ¡ Good reliability"
            else:
                assessment = "ğŸ”´ Poor reliability"
            
            report_lines.append(f"  - Reliability assessment: {assessment}")
            report_lines.append("")
    
    # ê¶Œì¥ì‚¬í•­
    report_lines.append("ğŸ’¡ RECOMMENDATIONS")
    report_lines.append("-" * 50)
    
    # ê°€ì¥ ìì£¼ ë“±ì¥í•˜ëŠ” ëª¨ë¸ ì¶”ì²œ
    most_frequent_model = sorted_models[0][0]
    most_frequent_count = sorted_models[0][1]
    
    report_lines.append(f"1. ğŸ… Most Consistent Performer: {most_frequent_model}")
    report_lines.append(f"   - Appeared in top-3 across {most_frequent_count}/{len(validation_methods)} validation methods")
    report_lines.append(f"   - Average rank: {np.mean(model_rank_positions[most_frequent_model]):.2f}")
    report_lines.append("")
    
    # ê°€ì¥ ì‹ ë¢°í•  ë§Œí•œ ê²€ì¦ ë°©ì‹ ì¶”ì²œ
    validation_reliability = {}
    for validation_type, ranks in top_models.items():
        differences = [abs(model_info['val_auc'] - model_info['test_auc']) 
                      for model_info in ranks.values()]
        validation_reliability[validation_type] = np.mean(differences)
    
    most_reliable_validation = min(validation_reliability.items(), key=lambda x: x[1])
    
    if most_reliable_validation[0] == 'real_validation':
        reliable_label = 'Real Anomaly Validation'
    elif most_reliable_validation[0].startswith('synthetic_'):
        anomaly_type = most_reliable_validation[0].replace('synthetic_', '').replace('_validation', '')
        reliable_label = f'Synthetic {anomaly_type.capitalize()} Validation'
    else:
        reliable_label = most_reliable_validation[0]
    
    report_lines.append(f"2. ğŸ¯ Most Reliable Validation Method: {reliable_label}")
    report_lines.append(f"   - Average val-test AUC difference: {most_reliable_validation[1]:.4f}")
    report_lines.append("")
    
    report_lines.append("3. ğŸ“‹ General Guidelines:")
    report_lines.append("   - Consider ensemble methods using top-3 models for robust performance")
    report_lines.append("   - Monitor validation-test consistency when selecting final model")
    report_lines.append("   - Use multiple validation strategies for comprehensive evaluation")
    report_lines.append("")
    
    report_lines.append("=" * 80)
    
    # ë¦¬í¬íŠ¸ ì €ì¥
    report_content = "\n".join(report_lines)
    report_filename = os.path.join(results_dir, 'top3_analysis_summary_report.txt')
    
    with open(report_filename, 'w', encoding='utf-8') as f:
        f.write(report_content)
    
    print(f"Top-3 ë¶„ì„ ìš”ì•½ ë¦¬í¬íŠ¸ê°€ {report_filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤")
    
    # ë¦¬í¬íŠ¸ ë‚´ìš©ì„ ì½˜ì†”ì—ë„ ì¶œë ¥
    print("\n" + report_content)