import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os

def convert_validation_labels(validation_types):
    """ê²€ì¦ íƒ€ì…ì„ í‘œì‹œìš© ë ˆì´ë¸”ë¡œ ë³€í™˜"""
    display_labels = []
    for vtype in validation_types:
        if vtype == 'real_validation':
            display_labels.append('GT Real Anomaly')
        elif vtype.startswith('synthetic_'):
            anomaly_type = vtype.replace('synthetic_', '').replace('_validation', '')
            display_labels.append(f'Synthetic {anomaly_type.capitalize()}')
        else:
            display_labels.append(vtype.replace('_', ' ').title())
    return display_labels

def plot_core_performance_metrics(evaluation_metrics, best_models, results_dir):
    """
    í•µì‹¬ ì„±ëŠ¥ ë©”íŠ¸ë¦­ë“¤ ì‹œê°í™”
    MSE: 5ê°œ ë°©ë²• ëª¨ë‘ (val_auc vs test_auc ì°¨ì´)
    ë‚˜ë¨¸ì§€: synthetic 4ê°œë§Œ (real_validationê³¼ ë¹„êµ)
    """
    if not evaluation_metrics and not best_models:
        print("âš ï¸ í‰ê°€ ë©”íŠ¸ë¦­ì´ ì—†ì–´ ì‹œê°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return
    
    # 2x2 ì„œë¸Œí”Œë¡¯
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    axes = axes.flatten()
    
    # 1. MSE (Best Model) - 5ê°œ ë°©ë²• ëª¨ë‘
    if best_models:
        validation_types_all = list(best_models.keys())
        display_labels_all = convert_validation_labels(validation_types_all)
        
        mse_values = []
        for vtype in validation_types_all:
            info = best_models[vtype]
            val_test_diff = info['val_auc'] - info['test_auc']
            mse_values.append(val_test_diff ** 2)  # MSEëŠ” ì°¨ì´ì˜ ì œê³±
        
        bars = axes[0].bar(display_labels_all, mse_values, alpha=0.8, color='lightcoral', 
                          edgecolor='black', linewidth=0.5)
        axes[0].set_title('MSE (Val AUC - Test AUC)', fontsize=13, fontweight='bold', pad=15)
        axes[0].set_ylabel('MSE Score', fontsize=11)
        axes[0].tick_params(axis='x', rotation=45, labelsize=10)
        axes[0].grid(True, alpha=0.3, linestyle=':')
        
        # ë°” ìœ„ì— ìˆ˜ì¹˜ í‘œì‹œ
        for bar, value in zip(bars, mse_values):
            height = bar.get_height()
            axes[0].text(bar.get_x() + bar.get_width()/2., height + height * 0.05,
                        f'{value:.4f}', ha='center', va='bottom', 
                        fontsize=9, fontweight='bold')
    else:
        axes[0].text(0.5, 0.5, 'No MSE Data', ha='center', va='center', 
                    transform=axes[0].transAxes, fontsize=12)
        axes[0].set_title('MSE (Val AUC - Test AUC)', fontsize=13, fontweight='bold')
    
    # 2-4. ë‚˜ë¨¸ì§€ ë©”íŠ¸ë¦­ë“¤ - Synthetic 4ê°œë§Œ
    if evaluation_metrics:
        validation_types_syn = list(evaluation_metrics.keys())
        display_labels_syn = convert_validation_labels(validation_types_syn)
        
        metrics_config = [
            {
                'name': 'Rank Correlation',
                'values': [evaluation_metrics[vtype]['rank_correlation'] for vtype in validation_types_syn],
                'color': 'lightgreen',
                'ylabel': 'Correlation',
                'ylim': (-0.1, 1.1)
            },
            {
                'name': 'Top-3 Overlap',
                'values': [evaluation_metrics[vtype]['top3_overlap'] for vtype in validation_types_syn],
                'color': 'gold',
                'ylabel': 'Overlap Ratio',
                'ylim': (0, 1.1)
            },
            {
                'name': 'Pairwise Win Rate',
                'values': [evaluation_metrics[vtype]['pairwise_win_rate'] for vtype in validation_types_syn],
                'color': 'mediumpurple',
                'ylabel': 'Win Rate',
                'ylim': (0, 1.1)
            }
        ]
        
        for idx, config in enumerate(metrics_config, 1):
            values = config['values']
            
            # NaN ê°’ ì²´í¬
            valid_values = [v for v in values if not np.isnan(v)]
            if not valid_values:
                axes[idx].text(0.5, 0.5, 'No Valid Data', ha='center', va='center', 
                              transform=axes[idx].transAxes, fontsize=12)
                axes[idx].set_title(config['name'], fontsize=13, fontweight='bold')
                continue
            
            # ë°” ê·¸ë˜í”„
            bars = axes[idx].bar(display_labels_syn, values, alpha=0.8, color=config['color'], 
                                edgecolor='black', linewidth=0.5)
            
            # ì¶• ì„¤ì •
            axes[idx].set_title(config['name'], fontsize=13, fontweight='bold', pad=15)
            axes[idx].set_ylabel(config['ylabel'], fontsize=11)
            axes[idx].tick_params(axis='x', rotation=45, labelsize=10)
            axes[idx].grid(True, alpha=0.3, linestyle=':')
            axes[idx].set_ylim(config['ylim'])
            
            # ë°” ìœ„ì— ìˆ˜ì¹˜ í‘œì‹œ
            for bar, value in zip(bars, values):
                if not np.isnan(value):
                    height = bar.get_height()
                    y_offset = 0.05 * (axes[idx].get_ylim()[1] - axes[idx].get_ylim()[0])
                    axes[idx].text(bar.get_x() + bar.get_width()/2., height + y_offset,
                                  f'{value:.3f}', ha='center', va='bottom', 
                                  fontsize=9, fontweight='bold')
    else:
        for idx in range(1, 4):
            axes[idx].text(0.5, 0.5, 'No Data', ha='center', va='center', 
                          transform=axes[idx].transAxes, fontsize=12)
    
    plt.suptitle('Synthetic Validation Performance Metrics', 
                 fontsize=15, fontweight='bold', y=0.98)
    plt.tight_layout()
    
    # ì €ì¥
    filename = os.path.join(results_dir, 'core_performance_metrics.png')
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"ğŸ“Š í•µì‹¬ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¹„êµ ì‹œê°í™”ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤")

def plot_best_model_test_performance(best_models, results_dir):
    """
    ê° ê²€ì¦ ë°©ì‹ë³„ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì˜ Validation vs Test ì„±ëŠ¥ ë¹„êµ
    ê° xì¶•ë§ˆë‹¤ 2ê°œì˜ ë§‰ëŒ€: Validation AUC vs Test AUC (performance drop í™•ì¸ìš©)
    """
    if not best_models:
        print("âš ï¸ Best ëª¨ë¸ ì •ë³´ê°€ ì—†ì–´ ì‹œê°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return

    # xì¶•ì— ëª¨ë“  validation ë°©ë²• í¬í•¨
    validation_types = list(best_models.keys())
    display_labels = convert_validation_labels(validation_types)
    
    # ë°ì´í„° ì¶”ì¶œ
    model_names = [info['model_name'] for info in best_models.values()]
    val_aucs = [info['val_auc'] for info in best_models.values()]
    test_aucs = [info['test_auc'] for info in best_models.values()]
    val_aps = [info['val_ap'] for info in best_models.values()]
    test_aps = [info['test_ap'] for info in best_models.values()]
    val_fdrs = [info.get('val_fdr', 0) for info in best_models.values()]  # Validation FDR ì¶”ê°€
    test_fdrs = [info.get('test_fdr', 0) for info in best_models.values()]
    
    # 1x3 ì„œë¸Œí”Œë¡¯
    fig, axes = plt.subplots(1, 3, figsize=(20, 7))
    
    # xì¶• ìœ„ì¹˜ ì„¤ì • (ê° ë°©ë²•ë§ˆë‹¤ 2ê°œ ë§‰ëŒ€)
    x = np.arange(len(validation_types))
    width = 0.35  # ë§‰ëŒ€ ë„ˆë¹„
    
    # AUC ë¹„êµ (Validation vs Test)
    bars1_val = axes[0].bar(x - width/2, val_aucs, width, label='Validation AUC', 
                           color='lightblue', alpha=0.8, edgecolor='black')
    bars1_test = axes[0].bar(x + width/2, test_aucs, width, label='Test AUC', 
                            color='orange', alpha=0.8, edgecolor='black')
    
    axes[0].set_title('ğŸ† Validation vs Test AUC (Best Models)', fontsize=13, fontweight='bold')
    axes[0].set_ylabel('AUC Score', fontsize=11)
    axes[0].set_xticks(x)
    axes[0].set_xticklabels(display_labels, rotation=45, ha='right')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3, linestyle=':')
    
    # AP ë¹„êµ (Validation vs Test)
    bars2_val = axes[1].bar(x - width/2, val_aps, width, label='Validation AP', 
                           color='lightgreen', alpha=0.8, edgecolor='black')
    bars2_test = axes[1].bar(x + width/2, test_aps, width, label='Test AP', 
                            color='red', alpha=0.8, edgecolor='black')
    
    axes[1].set_title('ğŸ¯ Validation vs Test AP (Best Models)', fontsize=13, fontweight='bold')
    axes[1].set_ylabel('AP Score', fontsize=11)
    axes[1].set_xticks(x)
    axes[1].set_xticklabels(display_labels, rotation=45, ha='right')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3, linestyle=':')
    
    # Test FDR (2ê°œ ë§‰ëŒ€: Validation FDR vs Test FDR)
    bars3_val = axes[2].bar(x - width/2, val_fdrs, width, label='Validation FDR', 
                           color='lightcoral', alpha=0.8, edgecolor='black')
    bars3_test = axes[2].bar(x + width/2, test_fdrs, width, label='Test FDR', 
                            color='darkred', alpha=0.8, edgecolor='black')
    
    axes[2].set_title('ğŸ“‰ Validation vs Test FDR (Best Models)', fontsize=13, fontweight='bold')
    axes[2].set_ylabel('FDR', fontsize=11)
    axes[2].set_xticks(x)
    axes[2].set_xticklabels(display_labels, rotation=45, ha='right')
    axes[2].legend()
    axes[2].grid(True, alpha=0.3, linestyle=':')
    max_fdr = max(val_fdrs + test_fdrs) if max(val_fdrs + test_fdrs) > 0 else 0.1
    axes[2].set_ylim(0, max_fdr * 1.2)
    
    # ìˆ˜ì¹˜ í‘œì‹œ
    # AUC ì°¨íŠ¸
    for i, (val_bar, test_bar, val_val, test_val, name) in enumerate(zip(bars1_val, bars1_test, val_aucs, test_aucs, model_names)):
        # Validation AUC ìˆ˜ì¹˜
        axes[0].text(val_bar.get_x() + val_bar.get_width()/2., val_bar.get_height() + 0.01,
                    f'{val_val:.3f}', ha='center', va='bottom', fontsize=8, fontweight='bold')
        # Test AUC ìˆ˜ì¹˜
        axes[0].text(test_bar.get_x() + test_bar.get_width()/2., test_bar.get_height() + 0.01,
                    f'{test_val:.3f}', ha='center', va='bottom', fontsize=8, fontweight='bold')
        # ëª¨ë¸ëª… (xì¶• í•˜ë‹¨)
        axes[0].text(i, min(val_aucs + test_aucs) - (max(val_aucs + test_aucs) - min(val_aucs + test_aucs)) * 0.1,
                    name, ha='center', va='top', fontsize=9, fontweight='bold', rotation=0)
    
    # AP ì°¨íŠ¸
    for i, (val_bar, test_bar, val_val, test_val) in enumerate(zip(bars2_val, bars2_test, val_aps, test_aps)):
        # Validation AP ìˆ˜ì¹˜
        axes[1].text(val_bar.get_x() + val_bar.get_width()/2., val_bar.get_height() + 0.01,
                    f'{val_val:.3f}', ha='center', va='bottom', fontsize=8, fontweight='bold')
        # Test AP ìˆ˜ì¹˜
        axes[1].text(test_bar.get_x() + test_bar.get_width()/2., test_bar.get_height() + 0.01,
                    f'{test_val:.3f}', ha='center', va='bottom', fontsize=8, fontweight='bold')
    
    # FDR ì°¨íŠ¸
    for i, (val_bar, test_bar, val_val, test_val) in enumerate(zip(bars3_val, bars3_test, val_fdrs, test_fdrs)):
        # Validation FDR ìˆ˜ì¹˜
        axes[2].text(val_bar.get_x() + val_bar.get_width()/2., val_bar.get_height() + max_fdr * 0.02,
                    f'{val_val:.3f}', ha='center', va='bottom', fontsize=8, fontweight='bold')
        # Test FDR ìˆ˜ì¹˜
        axes[2].text(test_bar.get_x() + test_bar.get_width()/2., test_bar.get_height() + max_fdr * 0.02,
                    f'{test_val:.3f}', ha='center', va='bottom', fontsize=8, fontweight='bold')
        # ëª¨ë¸ëª… (xì¶• í•˜ë‹¨)
        axes[2].text(i, -max_fdr * 0.1,
                    model_names[i], ha='center', va='top', fontsize=9, fontweight='bold', rotation=0)
    
    plt.suptitle('ğŸ” Best Model Performance: Validation vs Test (Performance Drop Analysis)', 
                 fontsize=16, fontweight='bold', y=0.98)
    plt.tight_layout()
    
    # ì €ì¥
    filename = os.path.join(results_dir, 'best_model_test_performance.png')
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"ğŸ† ìµœê³  ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ì‹œê°í™” (Performance Drop)ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤")

def plot_validation_test_correlation(summary_df, results_dir):
    """
    ê²€ì¦ ì„±ëŠ¥ê³¼ í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ê°„ì˜ ìƒê´€ê´€ê³„ ë¶„ì„ 
    GT ê°•ì¡°, ëª¨ë¸ ì´ë¦„ í‘œê¸°
    """
    if len(summary_df) == 0:
        print("âš ï¸ ìš”ì•½ ë°ì´í„°ê°€ ì—†ì–´ ìƒê´€ê´€ê³„ ì‹œê°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return
    
    validation_types = summary_df['validation_type'].unique()
    
    plt.figure(figsize=(14, 10))
    
    # ìƒ‰ìƒ ë° ë§ˆì»¤ ì„¤ì •
    colors = {
        'real_validation': 'red', 
        'synthetic_local_validation': 'blue', 
        'synthetic_cluster_validation': 'green', 
        'synthetic_global_validation': 'purple', 
        'synthetic_discrepancy_validation': 'orange'
    }
    
    markers = {
        'real_validation': 'o', 
        'synthetic_local_validation': '^', 
        'synthetic_cluster_validation': 's', 
        'synthetic_global_validation': 'D', 
        'synthetic_discrepancy_validation': '*'
    }
    
    for val_type in validation_types:
        df_subset = summary_df[summary_df['validation_type'] == val_type]
        df_subset = df_subset.dropna(subset=['val_auc', 'test_auc'])
        
        if len(df_subset) > 0:
            color = colors.get(val_type, 'gray')
            marker = markers.get(val_type, 'o')
            
            # ë ˆì´ë¸” ë° ìŠ¤íƒ€ì¼ ì„¤ì •
            if val_type == 'real_validation':
                label = 'GT Real Anomaly'
                alpha = 1.0
                size = 120
                zorder = 10
            else:
                anomaly_type = val_type.replace('synthetic_', '').replace('_validation', '')
                label = f'Synthetic {anomaly_type.capitalize()}'
                alpha = 0.7
                size = 80
                zorder = 5
            
            # ì‚°ì ë„
            scatter = plt.scatter(df_subset['val_auc'], df_subset['test_auc'],
                                alpha=alpha, label=label, color=color, marker=marker, s=size, 
                                edgecolors='black', linewidths=0.5, zorder=zorder)
            
            # ê° ì ì— ëª¨ë¸ ì´ë¦„(PCA, IForest ë“±) í‘œê¸°
            for _, row in df_subset.iterrows():
                plt.annotate(
                    row['model'],  # ëª¨ë¸ëª…: PCA, IForest, KNN, LOF ë“±
                    (row['val_auc'], row['test_auc']),
                    xytext=(3, 3),  # ì ì—ì„œ 3í”½ì…€ ë–¨ì–´ì§„ ìœ„ì¹˜ (ë” ê°€ê¹ê²Œ)
                    textcoords='offset points',
                    fontsize=7,
                    fontweight='bold',
                    color='black',  # ê²€ì€ìƒ‰ í…ìŠ¤íŠ¸ë¡œ ê°€ë…ì„± í–¥ìƒ
                    alpha=0.8,
                    bbox=dict(
                        boxstyle='round,pad=0.1', 
                        facecolor='white',      # í°ìƒ‰ ë°°ê²½
                        alpha=0.7, 
                        edgecolor=color,        # validation type ìƒ‰ìƒìœ¼ë¡œ í…Œë‘ë¦¬
                        linewidth=0.5
                    ),
                    ha='left',
                    va='bottom'
                )
    
    # ì™„ë²½í•œ ìƒê´€ê´€ê³„ ëŒ€ê°ì„ 
    min_val = summary_df[['val_auc', 'test_auc']].min().min()
    max_val = summary_df[['val_auc', 'test_auc']].max().max()
    plt.plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.5, 
             label='Perfect Correlation', linewidth=2)
    
    plt.xlabel('Validation AUC', fontsize=12, fontweight='bold')
    plt.ylabel('Test AUC', fontsize=12, fontweight='bold')
    plt.title('Validation vs Test Performance Correlation (with Model Names)', 
              fontsize=15, fontweight='bold')
    plt.legend(fontsize=10, loc='lower right', framealpha=0.9)
    plt.grid(True, linestyle=':', alpha=0.7)
    plt.tight_layout()
    
    # ì €ì¥
    filename = os.path.join(results_dir, 'validation_test_correlation.png')
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"ê²€ì¦-í…ŒìŠ¤íŠ¸ ìƒê´€ê´€ê³„ ê·¸ë˜í”„ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤")

def create_experiment_visualizations(best_models, evaluation_metrics, summary_df, results_dir):
    """
    ì‹¤í—˜ì˜ ëª¨ë“  í•µì‹¬ ì‹œê°í™”ë¥¼ ìƒì„±
    
    Parameters:
    - best_models: ê²€ì¦ ë°©ì‹ë³„ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì •ë³´ 
    - evaluation_metrics: í‰ê°€ ë©”íŠ¸ë¦­ ê²°ê³¼ 
    - summary_df: ì „ì²´ ì‹¤í—˜ ê²°ê³¼ DataFrame
    - results_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
    """
    print(f"\nğŸ¨ í•µì‹¬ ì‹œê°í™” ìƒì„± ì¤‘... ")
    
    # 1. í•µì‹¬ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¹„êµ
    if evaluation_metrics or best_models:
        plot_core_performance_metrics(evaluation_metrics, best_models, results_dir)
    else:
        print("âš ï¸ í‰ê°€ ë©”íŠ¸ë¦­ì´ ì—†ì–´ í•µì‹¬ ë©”íŠ¸ë¦­ ì‹œê°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
    
    # 2. Best ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ë¹„êµ (ê¸°ì¤€ì„  ì—†ìŒ)
    if best_models:
        plot_best_model_test_performance(best_models, results_dir)
    else:
        print("âš ï¸ Best ëª¨ë¸ ì •ë³´ê°€ ì—†ì–´ ì„±ëŠ¥ ë¹„êµ ì‹œê°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
    
    # 3. ê²€ì¦-í…ŒìŠ¤íŠ¸ ìƒê´€ê´€ê³„ (GT ê°•ì¡°, ëª¨ë¸ëª… í‘œê¸°)
    if len(summary_df) > 0:
        plot_validation_test_correlation(summary_df, results_dir)
    else:
        print("âš ï¸ ìš”ì•½ ë°ì´í„°ê°€ ì—†ì–´ ìƒê´€ê´€ê³„ ì‹œê°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
    
    print(f"âœ… ëª¨ë“  í•µì‹¬ ì‹œê°í™” ì™„ë£Œ!")
    print(f"ğŸ“ ì‹œê°í™” íŒŒì¼ë“¤ì´ {results_dir}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤")
    print(f"ğŸ“Š ìƒì„±ëœ íŒŒì¼:")
    print(f"   - core_performance_metrics.png")
    print(f"   - best_model_test_performance.png")
    print(f"   - validation_test_correlation.png")