import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
import os
from datetime import datetime
from sklearn.model_selection import train_test_split
from data_generator import SimpleDataGenerator
from llm_anomaly_generator import LLMAnomalyGenerator
from model_selection_enhanced import run_model_selection_experiment
import argparse
import sys
import time

# ì„¤ì •
RANDOM_SEED = 42
ANOMALY_TYPES = ['local', 'cluster', 'global', 'discrepancy']

class Tee(object):
    """í„°ë¯¸ë„ê³¼ íŒŒì¼ ë™ì‹œ ì¶œë ¥ì„ ìœ„í•œ í´ë˜ìŠ¤"""
    def __init__(self, *files):
        self.files = files
    def write(self, obj):
        for f in self.files:
            f.write(obj)
            f.flush()
    def flush(self):
        for f in self.files:
            f.flush()

def load_dataset(dataset_name):
    """CSV ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬"""
    csv_path = f'/lab-di/nfsdata/home/suhee.yoon/autoanoeval/data/adbench_column/{dataset_name}.csv'    
    print(f"ğŸ“¥ CSV ë°ì´í„°ì…‹ ë¡œë“œ: {csv_path}")
    
    if not os.path.exists(csv_path):
        print(f"âŒ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {csv_path}")
        raise FileNotFoundError(f"Dataset file not found: {csv_path}")
    
    df = pd.read_csv(csv_path)
    print(f"ğŸ“Š ë¡œë“œëœ ë°ì´í„° í˜•íƒœ: {df.shape}")

    y = df['label'].values
    df = df.drop(columns=['label'])

    # ìˆ«ì / ë¬¸ìì—´ feature ìë™ êµ¬ë¶„
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    string_cols = df.select_dtypes(include=['object', 'string']).columns
    
    print(f"ğŸ“ˆ ìˆ«ì ì»¬ëŸ¼: {len(numeric_cols)}ê°œ, ë¬¸ìì—´ ì»¬ëŸ¼: {len(string_cols)}ê°œ")

    # ìˆ«ì feature normalize
    scaler = MinMaxScaler()
    X_numeric = scaler.fit_transform(df[numeric_cols].values)

    # ë¬¸ìì—´ feature â†’ integer encoding
    X_strings = []
    for col in string_cols:
        unique_vals = np.unique(df[col])
        val_to_int = {val: idx for idx, val in enumerate(unique_vals)}
        encoded_col = np.vectorize(val_to_int.get)(df[col].values).reshape(-1, 1)
        X_strings.append(encoded_col)
        print(f"   ğŸ”¤ {col}: {len(unique_vals)}ê°œ ê³ ìœ ê°’ â†’ ì •ìˆ˜ ì¸ì½”ë”©")

    # ìˆ«ì + ë¬¸ìì—´ feature ê²°í•©
    if X_strings:
        X_strings = np.hstack(X_strings)
        X = np.hstack([X_numeric, X_strings])
        print(f"ğŸ“Š ìµœì¢… feature ì°¨ì›: {X_numeric.shape[1]} (ìˆ«ì) + {X_strings.shape[1]} (ë¬¸ìì—´) = {X.shape[1]}")
    else:
        X = X_numeric
        print(f"ğŸ“Š ìµœì¢… feature ì°¨ì›: {X.shape[1]} (ìˆ«ìë§Œ)")

    print(f"ğŸ·ï¸ ë ˆì´ë¸” ë¶„í¬: ì •ìƒ {np.sum(y == 0):,}ê°œ, ì´ìƒ {np.sum(y == 1):,}ê°œ")
    
    return X, y

def prepare_dataset_splits(X_original, y_original):
    """ë°ì´í„°ì…‹ ë¶„í• """
    print(f"ğŸ“Š ì›ë³¸ ë°ì´í„°: {X_original.shape}")
    print(f"ğŸ“Š í´ë˜ìŠ¤ ë¶„í¬ - ì •ìƒ: {np.sum(y_original == 0):,}, ì´ìƒ: {np.sum(y_original == 1):,}")

    # ëŒ€ìš©ëŸ‰ ë°ì´í„° ìµœì í™”
    max_normal = 3000
    max_anomaly = 500
    
    if np.sum(y_original == 0) > max_normal * 2:
        print(f"âš¡ ëŒ€ìš©ëŸ‰ ì •ìƒ ë°ì´í„° ê°ì§€. {max_normal:,}ê°œë¡œ ì œí•œ")
    if np.sum(y_original == 1) > max_anomaly * 2:
        print(f"âš¡ ëŒ€ìš©ëŸ‰ ì´ìƒ ë°ì´í„° ê°ì§€. {max_anomaly:,}ê°œë¡œ ì œí•œ")

    # ë°ì´í„° ì œí•œ ë° ë¶„ë¦¬
    X_normal = X_original[y_original == 0][:max_normal]
    X_anomaly = X_original[y_original == 1][:max_anomaly]
    
    # ë°ì´í„° ë¶„í• 
    X_normal_train, X_normal_holdout = train_test_split(
        X_normal, test_size=0.4, random_state=RANDOM_SEED
    )
    X_anomaly_val, X_anomaly_test = train_test_split(
        X_anomaly, test_size=0.7, random_state=RANDOM_SEED
    )
    X_normal_val, X_normal_test = train_test_split(
        X_normal_holdout, test_size=0.5, random_state=RANDOM_SEED
    )
    
    # ìµœì¢… ë°ì´í„°ì…‹ êµ¬ì„±
    X_val_real = np.vstack([X_normal_val, X_anomaly_val])
    y_val_real = np.concatenate([np.zeros(len(X_normal_val)), np.ones(len(X_anomaly_val))])
    
    X_test = np.vstack([X_normal_test, X_anomaly_test])
    y_test = np.concatenate([np.zeros(len(X_normal_test)), np.ones(len(X_anomaly_test))])
    
    # ë°ì´í„° ì…”í”Œ
    idx = np.random.RandomState(RANDOM_SEED).permutation(len(y_val_real))
    X_val_real, y_val_real = X_val_real[idx], y_val_real[idx]
    
    idx = np.random.RandomState(RANDOM_SEED).permutation(len(y_test))
    X_test, y_test = X_test[idx], y_test[idx]
    
    print(f"\nğŸ“‹ ë°ì´í„°ì…‹ ë¶„í•  ì™„ë£Œ:")
    print(f"   Train (ì •ìƒë§Œ): {X_normal_train.shape}")
    print(f"   Real Validation: {X_val_real.shape} (ì •ìƒ: {np.sum(y_val_real == 0):,}, ì´ìƒ: {np.sum(y_val_real == 1):,})")
    print(f"   Test: {X_test.shape} (ì •ìƒ: {np.sum(y_test == 0):,}, ì´ìƒ: {np.sum(y_test == 1):,})")
    
    return X_normal_train, X_normal_val, X_val_real, y_val_real, X_test, y_test, X_anomaly_val

def generate_llm_validation_sets(X_original, y_original, X_normal_val, X_anomaly_val, 
                                feature_names=None, dataset_name="Unknown", 
                                openai_api_key=None):
    """LLM ê¸°ë°˜ í•©ì„± ì´ìƒì¹˜ ê²€ì¦ ì„¸íŠ¸ ìƒì„±"""
    print(f"\nğŸ§ª LLM ê¸°ë°˜ ì´ìƒì¹˜ íŒ¨í„´ ë¶„ì„ ë° ìƒì„±...")
    
    if not openai_api_key:
        print("âŒ OpenAI API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. í†µê³„ì  ë°©ë²•ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
        return {}
    
    # LLM ìƒì„±ê¸° ì´ˆê¸°í™”
    llm_generator = LLMAnomalyGenerator(api_key=openai_api_key)
    
    try:
        # LLM ê¸°ë°˜ ì´ìƒì¹˜ ìƒì„±
        synthetic_anomalies = llm_generator.generate_anomalies(
            X=X_original,
            y=y_original,
            anomaly_count=len(X_anomaly_val),
            feature_names=feature_names,
            dataset_name=dataset_name
        )
        
        if len(synthetic_anomalies) > 0:
            # ê²€ì¦ ì„¸íŠ¸ êµ¬ì„±
            X_val_synthetic = np.vstack([X_normal_val, synthetic_anomalies])
            y_val_synthetic = np.concatenate([
                np.zeros(len(X_normal_val)), 
                np.ones(len(synthetic_anomalies))
            ])
            
            # ë°ì´í„° ì…”í”Œ
            idx = np.random.RandomState(RANDOM_SEED).permutation(len(y_val_synthetic))
            X_val_synthetic, y_val_synthetic = X_val_synthetic[idx], y_val_synthetic[idx]
            
            print(f"âœ… LLM ê¸°ë°˜ ê²€ì¦ ì„¸íŠ¸ ìƒì„± ì™„ë£Œ: {X_val_synthetic.shape}")
            print(f"   ì •ìƒ: {np.sum(y_val_synthetic == 0):,}, ì´ìƒ: {np.sum(y_val_synthetic == 1):,}")
            
            return {"llm_patterns": (X_val_synthetic, y_val_synthetic)}
        else:
            print("âŒ LLM ì´ìƒì¹˜ ìƒì„± ì‹¤íŒ¨")
            return {}
            
    except Exception as e:
        print(f"âŒ LLM ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        return {}

def generate_synthetic_validation_sets(X_original, y_original, X_normal_val, X_anomaly_val):
    """ê¸°ì¡´ í†µê³„ì  í•©ì„± ì´ìƒì¹˜ ê²€ì¦ ì„¸íŠ¸ ìƒì„± (í´ë°±ìš©)"""
    print(f"\nğŸ§ª í†µê³„ì  Synthetic Anomaly ê²€ì¦ ì„¸íŠ¸ ìƒì„±...")
    
    # ë°ì´í„° ìƒì„±ê¸° ì´ˆê¸°í™”
    data_generator = SimpleDataGenerator(seed=RANDOM_SEED)
    synthetic_val_sets = {}
    
    # ê° ìœ í˜•ë³„ í•©ì„± ì´ìƒì¹˜ ìƒì„±
    for anomaly_type in ANOMALY_TYPES:
        print(f"   ğŸ”¬ {anomaly_type} ìœ í˜• ìƒì„± ì¤‘...")
        
        try:
            synthetic_anomalies = data_generator.generate_anomalies(
                X=X_original,
                y=y_original,
                anomaly_type=anomaly_type,
                alpha=5,
                percentage=0.2,
                anomaly_count=len(X_anomaly_val)
            )
            
            # ê²€ì¦ ì„¸íŠ¸ êµ¬ì„±
            X_val_synthetic = np.vstack([X_normal_val, synthetic_anomalies])
            y_val_synthetic = np.concatenate([np.zeros(len(X_normal_val)), np.ones(len(synthetic_anomalies))])
            
            # ë°ì´í„° ì…”í”Œ
            idx = np.random.RandomState(RANDOM_SEED).permutation(len(y_val_synthetic))
            X_val_synthetic, y_val_synthetic = X_val_synthetic[idx], y_val_synthetic[idx]
            
            synthetic_val_sets[anomaly_type] = (X_val_synthetic, y_val_synthetic)
            
            print(f"      âœ… {anomaly_type}: {X_val_synthetic.shape} "
                  f"(ì •ìƒ: {np.sum(y_val_synthetic == 0):,}, ì´ìƒ: {np.sum(y_val_synthetic == 1):,})")
            
        except Exception as e:
            print(f"      âŒ {anomaly_type} ìƒì„± ì‹¤íŒ¨: {e}")
    
    return synthetic_val_sets

def main(args):
    """ë©”ì¸ ì‹¤í—˜ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ”¬ LLM ê¸°ë°˜ ì´ìƒì¹˜ íŒ¨í„´ ë¶„ì„ ì‹¤í—˜ ì‹œì‘")
    print("=" * 80)
    
    # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_dir = f"./result_metric/{args.dataset_name}_experiment_results_{timestamp}"
    os.makedirs(results_dir, exist_ok=True)

    # ë¡œê·¸ íŒŒì¼ ì„¤ì •
    log_file = open(os.path.join(results_dir, "experiment_log.txt"), "w")
    sys.stdout = Tee(sys.stdout, log_file)
    sys.stderr = Tee(sys.stderr, log_file)
    
    try:
        # 1. ë°ì´í„°ì…‹ ì¤€ë¹„ (CSV ë¡œë“œ)
        print(f"\nğŸ“Š ë°ì´í„°ì…‹ ë¡œë“œ: {args.dataset_name}")
        X_original, y_original = load_dataset(args.dataset_name)
        
        # 2. ë°ì´í„°ì…‹ ë¶„í• 
        experiment_start_time = time.time()
        
        X_normal_train, X_normal_val, X_val_real, y_val_real, X_test, y_test, X_anomaly_val = prepare_dataset_splits(
            X_original, y_original
        )
        
        # íŠ¹ì„± ì´ë¦„ ìƒì„± (ì‹¤ì œ ë°ì´í„°ì— ë§ê²Œ ìˆ˜ì • í•„ìš”)
        feature_names = [f"Feature_{i}" for i in range(X_original.shape[1])]
        
        # 3. LLM ê¸°ë°˜ ì´ìƒì¹˜ íŒ¨í„´ ë¶„ì„ ë° ìƒì„±
        llm_val_sets = generate_llm_validation_sets(
            X_original=X_original, 
            y_original=y_original, 
            X_normal_val=X_normal_val, 
            X_anomaly_val=X_anomaly_val,
            feature_names=feature_names,
            dataset_name=args.dataset_name,
            openai_api_key=getattr(args, 'openai_api_key', None)
        )
        
        # 4. ê¸°ì¡´ í†µê³„ì  ë°©ë²•ê³¼ ë³‘í–‰ ì‚¬ìš©ë„ ê°€ëŠ¥
        if not llm_val_sets and hasattr(args, 'use_statistical_fallback') and args.use_statistical_fallback:
            print("\nğŸ”„ í†µê³„ì  ë°©ë²•ìœ¼ë¡œ í´ë°±...")
            synthetic_val_sets = generate_synthetic_validation_sets(
                X_original, y_original, X_normal_val, X_anomaly_val
            )
        else:
            synthetic_val_sets = llm_val_sets
        
        print(f"\nğŸ“‹ ìƒì„±ëœ ê²€ì¦ ì„¸íŠ¸: {list(synthetic_val_sets.keys())}")
        
        # 5. ëª¨ë¸ ì„ íƒ ì‹¤í—˜ ì‹¤í–‰ (ëª¨ë“ˆì´ ìˆëŠ” ê²½ìš°)
        try:
            print(f"\nğŸš€ ëª¨ë¸ ì„ íƒ ì‹¤í—˜ ì‹¤í–‰...")
            model_start_time = time.time()
            
            all_results, best_models, evaluation_metrics = run_model_selection_experiment(
                X_normal_train=X_normal_train,
                X_val_real=X_val_real,
                y_val_real=y_val_real,
                synthetic_val_sets=synthetic_val_sets,
                X_test=X_test,
                y_test=y_test,
                results_dir=results_dir
            )
            
            model_time = time.time() - model_start_time
            total_time = time.time() - experiment_start_time
            
            # ì„±ëŠ¥ ìš”ì•½
            print(f"\nâ±ï¸ ì‹¤í—˜ ì‹œê°„ ìš”ì•½:")
            print(f"   ë°ì´í„° ì¤€ë¹„: {model_start_time - experiment_start_time:.2f}s")
            print(f"   ëª¨ë¸ í•™ìŠµ/í‰ê°€: {model_time:.2f}s")
            print(f"   ì „ì²´ ì‹¤í—˜ ì‹œê°„: {total_time:.2f}s")
            
        except ImportError:
            print(f"âš ï¸ model_selection_enhanced ëª¨ë“ˆì´ ì—†ìŠµë‹ˆë‹¤. íŒ¨í„´ ë¶„ì„ê¹Œì§€ë§Œ ìˆ˜í–‰ë©ë‹ˆë‹¤.")
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ì„ íƒ ì‹¤í—˜ ì‹¤íŒ¨: {e}")
        
        print(f"\nğŸ‰ ì‹¤í—˜ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ!")
        print(f"ğŸ“ ê²°ê³¼ ìœ„ì¹˜: {results_dir}")
        
    except Exception as e:
        print(f"\nâŒ ì‹¤í—˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise
    finally:
        log_file.close()

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="LLM ê¸°ë°˜ ì´ìƒì¹˜ íŒ¨í„´ ë¶„ì„ ì‹¤í—˜")
    parser.add_argument("--dataset_name", type=str, required=True, 
                       help="ë°ì´í„°ì…‹ ì´ë¦„")
    parser.add_argument("--openai_api_key", type=str, required=True,
                       help="OpenAI API í‚¤")
    parser.add_argument("--use_statistical_fallback", action="store_true", default=False,
                       help="LLM ì‹¤íŒ¨ ì‹œ í†µê³„ì  ë°©ë²• ì‚¬ìš©")
    
    args = parser.parse_args()
    main(args)