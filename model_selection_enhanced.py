import numpy as np
import pandas as pd
import os

from model_evaluator import get_default_models, prepare_data, evaluate_models, get_best_model_info, filter_valid_results
from metrics_calculator import calculate_evaluation_metrics
from visualization_utils import create_experiment_visualizations

def run_model_selection_experiment(X_normal_train, X_val_real, y_val_real, 
                                   synthetic_val_sets, X_test, y_test, results_dir):
    """
    ğŸ”¬ Synthetic Anomaly ê¸°ë°˜ ëª¨ë¸ ì„ íƒì˜ ì‹¤ìš©ì„± ê²€ì¦ ì‹¤í—˜
    
    í•µì‹¬ ì§ˆë¬¸: Synthetic anomalyë¡œ ì„ íƒí•œ best modelì´ 
              Real anomaly testì—ì„œë„ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ”ê°€?
    
    Returns:
        tuple: (all_results, best_models, evaluation_metrics)
    """
    print("\n" + "="*60)
    print("ğŸ”¬ ëª¨ë¸ ì„ íƒ ì‹¤í—˜ ì‹¤í–‰")
    print("="*60)
    
    # 1. ì‹¤í—˜ ì¤€ë¹„
    print(f"\nğŸ“Š ë°ì´í„° ì¤€ë¹„ ë° í‘œì¤€í™”...")
    X_normal_train_scaled, X_val_real_scaled, X_test_scaled, synthetic_val_sets_scaled = prepare_data(
        X_normal_train, X_val_real, X_test, synthetic_val_sets
    )
    
    models = get_default_models()
    print(f"âœ… í‰ê°€ ëª¨ë¸: {len(models)}ê°œ")
    print(f"ğŸ“‹ ëª¨ë¸ ëª©ë¡: {list(models.keys())}")
    
    # 2. ì‹¤í—˜ ì‹¤í–‰
    all_results = {}
    
    # 2-1. GT Real Anomaly Validation (ê¸°ì¤€ì„ )
    print(f"\nğŸ¯ GT Real Anomaly Validation (ê¸°ì¤€ì„ )")
    real_results = evaluate_models(
        models, X_normal_train_scaled, X_val_real_scaled, y_val_real, X_test_scaled, y_test
    )
    all_results['real_validation'] = real_results
    _print_validation_summary('GT Real Anomaly', real_results)
    
    # 2-2. Synthetic Anomaly Validations
    print(f"\nğŸ§ª Synthetic Anomaly Validations")
    for anomaly_type, (X_val_syn, y_val_syn) in synthetic_val_sets_scaled.items():
        print(f"\n--- {anomaly_type.capitalize()} Synthetic ---")
        
        synthetic_results = evaluate_models(
            models, X_normal_train_scaled, X_val_syn, y_val_syn, X_test_scaled, y_test
        )
        all_results[f'synthetic_{anomaly_type}_validation'] = synthetic_results
        _print_validation_summary(f'Synthetic {anomaly_type}', synthetic_results)
    
    # 3. ê²°ê³¼ ë¶„ì„
    print(f"\nğŸ“ˆ ê²°ê³¼ ë¶„ì„ ë° ì €ì¥...")
    best_models, evaluation_metrics, summary_df = _analyze_and_save_results(all_results, results_dir)
    
    # 4. ì‹œê°í™” ìƒì„± (ìˆ˜ì •ëœ í•¨ìˆ˜ í˜¸ì¶œ)
    print(f"Debug - best_models keys: {list(best_models.keys())}")
    print(f"Debug - evaluation_metrics keys: {list(evaluation_metrics.keys())}")
    create_experiment_visualizations(best_models, evaluation_metrics, summary_df, results_dir)
    
    # 5. ìµœì¢… ë¦¬í¬íŠ¸
    _generate_summary_report(best_models, evaluation_metrics, results_dir)
    _print_experiment_conclusion(evaluation_metrics)
    
    return all_results, best_models, evaluation_metrics

def _print_validation_summary(validation_name, results):
    """ê²€ì¦ ê²°ê³¼ ê°„ë‹¨ ìš”ì•½ ì¶œë ¥"""
    valid_results = filter_valid_results(results)
    if not valid_results:
        print(f"âš ï¸  {validation_name}: ìœ íš¨í•œ ê²°ê³¼ ì—†ìŒ")
        return
    
    best_info = get_best_model_info(valid_results)
    if best_info:
        print(f"ğŸ† Best: {best_info['model_name']} (Test AUC: {best_info['test_auc']:.4f})")

def _analyze_and_save_results(all_results, results_dir):
    """ê²°ê³¼ ë¶„ì„ ë° íŒŒì¼ ì €ì¥"""
    summary_data = []
    best_models = {}
    evaluation_metrics = {}
    
    real_results = filter_valid_results(all_results['real_validation'])
    
    for validation_type, results in all_results.items():
        valid_results = filter_valid_results(results)
        if not valid_results:
            continue
        
        # Best ëª¨ë¸ ì •ë³´ ìˆ˜ì§‘
        best_info = get_best_model_info(valid_results)
        if best_info:
            best_models[validation_type] = best_info
        
        # í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚° (Realê³¼ ë¹„êµ, syntheticë§Œ)
        if validation_type != 'real_validation' and real_results:
            metrics = calculate_evaluation_metrics(real_results, valid_results)
            evaluation_metrics[validation_type] = metrics
            
            # ê°„ë‹¨í•œ ê²°ê³¼ ì¶œë ¥
            synthetic_type = validation_type.replace('synthetic_', '').replace('_validation', '')
            print(f"ğŸ“Š {synthetic_type}: Corr={metrics['rank_correlation']:.3f}, Overlap={metrics['top3_overlap']:.3f}")
        
        # ì „ì²´ ê²°ê³¼ ë°ì´í„° ìˆ˜ì§‘
        for model_name, model_metrics in results.items():
            summary_data.append({
                'validation_type': validation_type,
                'model': model_name,
                'val_auc': model_metrics['val_auc'],
                'test_auc': model_metrics['test_auc'],
                'val_ap': model_metrics['val_ap'],
                'test_ap': model_metrics['test_ap'],
                'test_fdr': model_metrics['test_fdr'],
                'training_time': model_metrics['training_time']
            })
    
    # ê²°ê³¼ íŒŒì¼ ì €ì¥
    summary_df = pd.DataFrame(summary_data)
    summary_df.to_csv(os.path.join(results_dir, 'model_selection_results.csv'), index=False)
    
    if evaluation_metrics:
        metrics_df = pd.DataFrame.from_dict(evaluation_metrics, orient='index')
        metrics_df.to_csv(os.path.join(results_dir, 'evaluation_metrics.csv'))
    
    print(f"ğŸ’¾ ê²°ê³¼ íŒŒì¼ ì €ì¥ ì™„ë£Œ")
    
    return best_models, evaluation_metrics, summary_df

def _generate_summary_report(best_models, evaluation_metrics, results_dir):
    """ì‹¤í—˜ ê²°ê³¼ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±"""
    report_path = os.path.join(results_dir, 'experiment_summary_report.txt')
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("ğŸ”¬ Synthetic Anomaly ê¸°ë°˜ ëª¨ë¸ ì„ íƒ ì‹¤ìš©ì„± ê²€ì¦ ì‹¤í—˜ ê²°ê³¼\n")
        f.write("=" * 80 + "\n\n")
        
        # ì‹¤í—˜ ëª©ì 
        f.write("ğŸ“‹ ì‹¤í—˜ ëª©ì \n")
        f.write("-" * 40 + "\n")
        f.write("Synthetic anomaly validationì´ real anomaly validationë§Œí¼\n")
        f.write("íš¨ê³¼ì ìœ¼ë¡œ best modelì„ ì„ íƒí•  ìˆ˜ ìˆëŠ”ì§€ ê²€ì¦\n\n")
        
        # Best ëª¨ë¸ ë¹„êµ
        f.write("ğŸ† ê° ê²€ì¦ ë°©ì‹ë³„ Best Model (Test Set ì„±ëŠ¥)\n")
        f.write("-" * 40 + "\n")
        
        gt_auc = None
        if 'real_validation' in best_models:
            gt_info = best_models['real_validation']
            gt_auc = gt_info['test_auc']
            f.write(f"GT Real Anomaly: {gt_info['model_name']} (AUC: {gt_auc:.4f}) [ê¸°ì¤€ì„ ]\n")
        
        for val_type, info in best_models.items():
            if val_type != 'real_validation':
                auc_diff = info['test_auc'] - gt_auc if gt_auc else 0
                synthetic_type = val_type.replace('synthetic_', '').replace('_validation', '')
                f.write(f"Synthetic {synthetic_type}: {info['model_name']} "
                       f"(AUC: {info['test_auc']:.4f}, ì°¨ì´: {auc_diff:+.4f})\n")
        
        # í•µì‹¬ í‰ê°€ ë©”íŠ¸ë¦­
        f.write(f"\nğŸ“Š í•µì‹¬ í‰ê°€ ë©”íŠ¸ë¦­ (1.0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ GTì™€ ìœ ì‚¬)\n")
        f.write("-" * 40 + "\n")
        
        if evaluation_metrics:
            for val_type, metrics in evaluation_metrics.items():
                synthetic_type = val_type.replace('synthetic_', '').replace('_validation', '')
                f.write(f"\nSynthetic {synthetic_type}:\n")
                f.write(f"  - Rank Correlation: {metrics['rank_correlation']:.4f}\n")
                f.write(f"  - Top-3 Overlap: {metrics['top3_overlap']:.4f}\n")
                f.write(f"  - Pairwise Win Rate: {metrics['pairwise_win_rate']:.4f}\n")
                f.write(f"  - MSE (Best Model): {metrics['mse_best_model']:.6f}\n")
            
            # ìµœê³  ì„±ëŠ¥ ì°¾ê¸°
            best_synthetic = max(evaluation_metrics.items(), 
                               key=lambda x: x[1]['rank_correlation'])
            best_type = best_synthetic[0].replace('synthetic_', '').replace('_validation', '')
            f.write(f"\nğŸ¥‡ ìµœê³  ì„±ëŠ¥: Synthetic {best_type} ")
            f.write(f"(Rank Correlation: {best_synthetic[1]['rank_correlation']:.4f})\n")
        
        # ê²°ë¡ 
        f.write(f"\nğŸ’¡ ì£¼ìš” ë°œê²¬ì‚¬í•­\n")
        f.write("-" * 40 + "\n")
        if evaluation_metrics:
            avg_correlation = np.mean([m['rank_correlation'] for m in evaluation_metrics.values()])
            avg_overlap = np.mean([m['top3_overlap'] for m in evaluation_metrics.values() 
                                 if not np.isnan(m['top3_overlap'])])
            avg_win_rate = np.mean([m['pairwise_win_rate'] for m in evaluation_metrics.values()])
            
            f.write(f"- í‰ê·  ìˆœìœ„ ìƒê´€ê´€ê³„: {avg_correlation:.4f}\n")
            f.write(f"- í‰ê·  Top-3 ì¼ì¹˜ìœ¨: {avg_overlap:.4f}\n")
            f.write(f"- í‰ê·  ìŒë³„ ì •í™•ë„: {avg_win_rate:.4f}\n\n")
            
            if avg_correlation >= 0.8:
                f.write("âœ… Synthetic validationì´ ë§¤ìš° íš¨ê³¼ì \n")
            elif avg_correlation >= 0.6:
                f.write("âš ï¸ Synthetic validationì´ ì–´ëŠ ì •ë„ íš¨ê³¼ì \n")
            else:
                f.write("âŒ Synthetic validationì˜ íš¨ê³¼ê°€ ì œí•œì \n")
        
        f.write(f"\nğŸ“ˆ ìƒì„¸ ê²°ê³¼ëŠ” CSV íŒŒì¼ê³¼ ì‹œê°í™”ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.\n")
    
    print(f"ğŸ“‹ ì‹¤í—˜ ìš”ì•½ ë¦¬í¬íŠ¸: {report_path}")

def _print_experiment_conclusion(evaluation_metrics):
    """ì‹¤í—˜ ê²°ë¡  ì¶œë ¥"""
    if not evaluation_metrics:
        return
    
    print("\n" + "="*60)
    print("ğŸ¯ ì‹¤í—˜ ê²°ë¡ ")
    print("="*60)
    
    # í‰ê·  ì„±ëŠ¥ ê³„ì‚°
    correlations = [m['rank_correlation'] for m in evaluation_metrics.values()]
    overlaps = [m['top3_overlap'] for m in evaluation_metrics.values() 
               if not np.isnan(m['top3_overlap'])]
    win_rates = [m['pairwise_win_rate'] for m in evaluation_metrics.values()]
    
    avg_corr = np.mean(correlations)
    avg_overlap = np.mean(overlaps) if overlaps else 0
    avg_win = np.mean(win_rates)
    
    print(f"ğŸ“Š ì „ì²´ í‰ê·  ì„±ëŠ¥:")
    print(f"   â€¢ ìˆœìœ„ ìƒê´€ê´€ê³„: {avg_corr:.4f}")
    print(f"   â€¢ Top-3 ì¼ì¹˜ìœ¨: {avg_overlap:.4f}")
    print(f"   â€¢ ìŒë³„ ì •í™•ë„: {avg_win:.4f}")
    
    # ìµœê³ /ìµœì € ì„±ëŠ¥
    best_method = max(evaluation_metrics.items(), key=lambda x: x[1]['rank_correlation'])
    worst_method = min(evaluation_metrics.items(), key=lambda x: x[1]['rank_correlation'])
    
    best_type = best_method[0].replace('synthetic_', '').replace('_validation', '')
    worst_type = worst_method[0].replace('synthetic_', '').replace('_validation', '')
    
    print(f"\nğŸ¥‡ ìµœê³ : Synthetic {best_type} (ìƒê´€ê´€ê³„: {best_method[1]['rank_correlation']:.4f})")
    print(f"ğŸ¥‰ ìµœì €: Synthetic {worst_type} (ìƒê´€ê´€ê³„: {worst_method[1]['rank_correlation']:.4f})")
    
    # ì‹¤ìš©ì„± í‰ê°€
    if avg_corr >= 0.8:
        conclusion = "âœ… ë§¤ìš° ì‹¤ìš©ì : Synthetic validation ì™„ì „ ëŒ€ì²´ ê°€ëŠ¥"
    elif avg_corr >= 0.6:
        conclusion = "âš ï¸ ì‹¤ìš©ì : Synthetic validation ë¶€ë¶„ì  í™œìš© ê°€ëŠ¥"
    else:
        conclusion = "âŒ ì œí•œì : Synthetic validation íš¨ê³¼ ë‚®ìŒ"
    
    print(f"\nğŸ’¡ ì¢…í•© í‰ê°€: {conclusion}")
    print("="*60)